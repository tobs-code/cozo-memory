# Framework Adapters Implementation Plan

## Research Summary (March 2026)

### LangChain Integration
**Key Interfaces:**
- `BaseChatMessageHistory` - Core interface for chat history storage
  - Methods: `add_messages()`, `aget_messages()`, `clear()`, `aclear()`
  - Async variants available for all methods
  - Bulk operations preferred over single-message methods
- `BaseMemory` - Higher-level memory abstraction
  - Uses `BaseChatMessageHistory` internally
  - Provides `save_context()` and `load_memory_variables()`

**Implementation Pattern:**
```python
from langchain.schema import BaseChatMessageHistory, BaseMessage
from typing import List

class CozoMemoryChatHistory(BaseChatMessageHistory):
    def __init__(self, session_id: str, mcp_client):
        self.session_id = session_id
        self.client = mcp_client
    
    def add_messages(self, messages: List[BaseMessage]) -> None:
        # Convert to observations and store via MCP
        pass
    
    async def aget_messages(self) -> List[BaseMessage]:
        # Query via MCP and convert back
        pass
    
    def clear(self) -> None:
        # Clear session via MCP
        pass
```

**Package Structure:**
```
cozo-memory-langchain/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chat_history.py      # BaseChatMessageHistory implementation
â”‚   â”œâ”€â”€ memory.py             # BaseMemory implementation  
â”‚   â”œâ”€â”€ retriever.py          # VectorStoreRetriever integration
â”‚   â””â”€â”€ client.py             # MCP client wrapper
â”œâ”€â”€ tests/
â”œâ”€â”€ examples/
â””â”€â”€ README.md
```

---

### CrewAI Integration
**Key Interfaces:**
- `Memory` class with `remember()`, `recall()`, `forget()` methods
- `StorageBackend` protocol for custom backends
- Embedder configuration via crew or memory instance
- Automatic fact extraction from task outputs

**Implementation Pattern:**
```python
from crewai.memory.storage.backend import StorageBackend

class CozoStorageBackend(StorageBackend):
    def __init__(self, mcp_client):
        self.client = mcp_client
    
    def save(self, scope: str, content: str, metadata: dict):
        # Store via MCP with scope as entity
        pass
    
    def search(self, query: str, scope: str = None) -> List[dict]:
        # Use hybrid search via MCP
        pass
    
    def delete(self, scope: str):
        # Delete entity via MCP
        pass
```

**Package Structure:**
```
cozo-memory-crewai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ storage_backend.py   # StorageBackend implementation
â”‚   â”œâ”€â”€ memory.py             # Memory wrapper
â”‚   â””â”€â”€ client.py             # MCP client wrapper
â”œâ”€â”€ tests/
â”œâ”€â”€ examples/
â””â”€â”€ README.md
```

**Integration Points:**
- Crew-level: `Crew(memory=Memory(storage=CozoStorageBackend()))`
- Agent-level: Scoped memory per agent
- Automatic embedder passthrough from crew config

---

### LlamaIndex Integration
**Key Interfaces:**
- `BasePydanticVectorStore` - Core vector store interface
  - Methods: `add()`, `delete()`, `query()`, `get()`
- `SimpleDocumentStore` - Document storage
- `SimpleIndexStore` - Index metadata storage
- `StorageContext` - Container for all storage components

**Implementation Pattern:**
```python
from llama_index.core.vector_stores import BasePydanticVectorStore
from llama_index.core.vector_stores import VectorStoreQuery, VectorStoreQueryResult

class CozoVectorStore(BasePydanticVectorStore):
    stores_text: bool = True
    
    def __init__(self, mcp_client):
        self.client = mcp_client
    
    def add(self, nodes: List[BaseNode]) -> List[str]:
        # Convert nodes to entities/observations via MCP
        pass
    
    def query(self, query: VectorStoreQuery) -> VectorStoreQueryResult:
        # Use hybrid search via MCP
        pass
    
    def delete(self, ref_doc_id: str) -> None:
        # Delete entity via MCP
        pass
```

**Package Structure:**
```
cozo-memory-llamaindex/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_store.py       # BasePydanticVectorStore implementation
â”‚   â”œâ”€â”€ doc_store.py          # Document store implementation
â”‚   â”œâ”€â”€ index_store.py        # Index store implementation
â”‚   â””â”€â”€ client.py             # MCP client wrapper
â”œâ”€â”€ tests/
â”œâ”€â”€ examples/
â””â”€â”€ README.md
```

---

## Implementation Priority

### Phase 1: LangChain Adapter âœ… (Week 1-2)
**Status:** Complete

**Deliverables:**
- âœ… `@cozo-memory/langchain` package
- âœ… `CozoMemoryChatHistory` implementation
- âœ… `CozoMemoryRetriever` implementation
- âœ… Session management integration
- âœ… Examples: chatbot, conversational RAG, graph-RAG
- âœ… Documentation

### Phase 2: LlamaIndex Adapter âœ… (Week 3-4)
**Status:** Complete

**Deliverables:**
- âœ… `@cozo-memory/llamaindex` package
- âœ… `CozoVectorStore` implementation
- âœ… Hybrid search and Graph-RAG support
- âœ… Persistent index storage
- âœ… Examples: basic-rag, graph-rag, persistent-index
- âœ… Documentation

### Phase 3: CrewAI Adapter â¸ï¸ (Postponed)
**Status:** Postponed - Awaiting TypeScript/Node.js support in CrewAI

**Reason:** CrewAI is currently Python-only. Will implement when official TypeScript SDK becomes available or community demand justifies a Python bridge package.

**Alternative:** Users can integrate via HTTP API bridge (`npm run bridge`) from Python CrewAI agents.

### Phase 3 (Alternative): Documentation & Publishing ðŸ“‹ (Week 5-6)
**Status:** In Progress

**Deliverables:**
- âœ… Comprehensive README updates
- ðŸ“‹ NPM package publishing setup
- ðŸ“‹ GitHub releases and tags
- ðŸ“‹ Example projects repository
- ðŸ“‹ Integration guides and tutorials
- ðŸ“‹ Performance benchmarks documentation

---

## Technical Architecture

### Shared MCP Client Layer
All adapters will use a common MCP client wrapper:

```python
# cozo-memory-adapters-core/
class CozoMemoryClient:
    """Shared MCP client for all framework adapters"""
    
    def __init__(self, server_path: str = None):
        # Connect to MCP server (stdio or HTTP bridge)
        pass
    
    async def create_entity(self, name, type, metadata):
        # MCP mutate_memory call
        pass
    
    async def add_observation(self, entity_id, text, metadata):
        # MCP mutate_memory call
        pass
    
    async def search(self, query, limit, filters):
        # MCP query_memory call
        pass
    
    async def hybrid_search(self, query, limit):
        # MCP query_memory with hybrid search
        pass
    
    async def graph_rag(self, query, max_depth):
        # MCP query_memory with graph_rag
        pass
```

### Conversion Utilities
Each adapter needs converters between framework types and cozo-memory types:

**LangChain:**
- `BaseMessage` â†” Observation
- Session ID â†” Entity

**LlamaIndex:**
- `BaseNode` â†” Entity/Observation
- `VectorStoreQuery` â†” MCP query params
- `VectorStoreQueryResult` â†” MCP search results

**CrewAI:**
- Memory record â†” Observation
- Scope â†” Entity hierarchy
- Categories â†” Metadata

---

## Success Metrics

### Adoption Metrics
- npm downloads per package
- GitHub stars/forks
- Community examples/tutorials

### Technical Metrics
- Test coverage > 80%
- Performance: < 100ms overhead vs native
- Compatibility: Support latest framework versions

### Documentation Metrics
- Complete API reference
- 3+ working examples per adapter
- Migration guides from native solutions

---


### Differentiation Points
- **Local-First:** No cloud dependencies
- **Graph-RAG:** Native graph traversal capabilities
- **Time-Travel:** Historical queries via Validity
- **Hybrid Search:** Vector + FTS + Graph in one query
- **Cost:** Free, no per-query pricing

---

## Next Steps

1. **Create mono-repo structure:**
   ```
   cozo-memory-adapters/
   â”œâ”€â”€ packages/
   â”‚   â”œâ”€â”€ core/              # Shared MCP client
   â”‚   â”œâ”€â”€ langchain/         # LangChain adapter
   â”‚   â”œâ”€â”€ llamaindex/        # LlamaIndex adapter
   â”‚   â””â”€â”€ crewai/            # CrewAI adapter
   â”œâ”€â”€ examples/
   â””â”€â”€ docs/
   ```

2. **Set up development environment:**
   - TypeScript/Python hybrid project
   - Shared testing infrastructure
   - CI/CD for all packages

3. **Start with LangChain adapter:**
   - Implement `BaseChatMessageHistory`
   - Add session management
   - Create 3 examples
   - Write documentation

4. **Iterate based on feedback:**
   - Monitor GitHub issues
   - Engage with early adopters
   - Refine based on real-world usage
