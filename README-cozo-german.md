# CozoDB Memory MCP Server (Archiv/Referenz)

[![npm](https://img.shields.io/npm/v/cozo-memory)](https://www.npmjs.com/package/cozo-memory)
[![Node](https://img.shields.io/node/v/cozo-memory)](https://nodejs.org)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue)](LICENSE)

> **Hinweis:** Dieses Repository wird prim√§r auf Englisch gepflegt. Die deutsche Dokumentation dient nur noch als Referenz und wird nicht mehr aktiv aktualisiert. Alle System-Komponenten (FTS, Keywords, Logs) sind nun auf Englisch optimiert.

**Local-first Memory f√ºr Claude & AI-Agenten mit Hybrid-Suche, Graph-RAG und Time-Travel ‚Äì alles in einer einzigen Binary, kein Cloud, kein Docker.**

## Inhaltsverzeichnis

- [Installation](#installation)
- [√úberblick](#√ºberblick)
- [Positionierung & Vergleich](#positionierung--vergleich)
- [Performance & Benchmarks](#performance--benchmarks)
- [Architektur](#architektur-high-level)
- [Start / Integration](#start--integration)
- [Konfiguration & Backends](#konfiguration--backends)
- [Datenmodell](#datenmodell)
- [MCP Tools](#mcp-tools)
  - [mutate_memory (Schreiben)](#mutate_memory-schreiben)
  - [query_memory (Lesen)](#query_memory-lesen)
  - [analyze_graph (Analyse)](#analyze_graph-analyse)
  - [manage_system (Wartung)](#manage_system-wartung)
- [Production Monitoring](#production-monitoring)
- [Technische Highlights](#technische-highlights)
- [Optional: HTTP API Bridge](#optional-http-api-bridge)
- [Entwicklung](#entwicklung)
- [User Preference Profiling](#user-preference-profiling-mem0-style)
- [Troubleshooting](#troubleshooting)
- [Lizenz](#lizenz)

## Installation

### Via npm (Empfohlen)

```bash
# Global installieren
npm install -g cozo-memory

# Oder direkt mit npx nutzen (keine Installation n√∂tig)
npx cozo-memory
```

### Aus dem Quellcode

```bash
git clone https://github.com/tobs-code/cozo-memory
cd cozo-memory
npm install && npm run build
npm run start
```

## √úberblick

üîç **Hybrid-Suche (seit v0.7)** - Kombination aus semantischer Suche (HNSW), Full-Text Search (FTS) und Graph-Signalen via Reciprocal Rank Fusion (RRF)

üï∏Ô∏è **Graph-RAG & Graph-Walking (seit v1.7)** - Erweitertes Retrieval mit Vektor-Seeds und rekursiven Graph-Traversals via optimierte Datalog-Algorithmen

üéØ **Multi-Vector Support (seit v1.7)** - Duale Embeddings pro Entity: Content-Embedding f√ºr Kontext, Name-Embedding f√ºr Identifikation

‚ö° **Semantic Caching (seit v0.8.5)** - Zweistufiger Cache (L1 Memory + L2 Persistent) mit semantischem Query-Matching

‚è±Ô∏è **Time-Travel-Abfragen** - Versionierung aller √Ñnderungen via CozoDB Validity; Abfragen zu jedem Zeitpunkt

üîó **Atomare Transaktionen (seit v1.2)** - Multi-Statement-Transaktionen f√ºr Datenkonsistenz

üìä **Graph-Algorithmen (seit v1.3/v1.6)** - PageRank, Betweenness Centrality, HITS, Community Detection, Shortest Path

üßπ **Janitor-Service** - LLM-gest√ºtzte automatische Bereinigung mit hierarchischer Summarization

üë§ **User Preference Profiling** - Persistente User-Pr√§ferenzen mit automatischem 50% Search-Boost

üîç **Near-Duplicate Detection** - Automatische LSH-basierte Deduplizierung zur Vermeidung von Redundanz

üß† **Inference Engine** - Implizite Wissensentdeckung mit mehreren Strategien

üè† **100% Lokal** - Embeddings via ONNX/Transformers; keine externen Services erforderlich

üì¶ **Export/Import (seit v1.8)** - Export nach JSON, Markdown oder Obsidian-ready ZIP; Import von Mem0, MemGPT, Markdown oder nativem Format

üìÑ **PDF-Unterst√ºtzung (seit v1.9)** - Direkte PDF-Ingestion mit Textextraktion via pdfjs-dist; unterst√ºtzt Dateipfad und Content-Parameter

üïê **Duales Zeitstempel-Format (seit v1.9)** - Alle Zeitstempel werden sowohl als Unix-Mikrosekunden als auch im ISO 8601 Format zur√ºckgegeben

### Detaillierte Features

Dieses Repository enth√§lt:
- einen MCP-Server (stdio) f√ºr Claude/andere MCP-Clients,
- einen optionalen HTTP-API-Bridge-Server f√ºr UI/Tools,

Wesentliche Eigenschaften:
- **Hybride Suche (v0.7 Optimized)**: Kombination aus semantischer Suche (HNSW), **Full-Text Search (FTS)** und Graph-Signalen, zusammengef√ºhrt via Reciprocal Rank Fusion (RRF).
- **Full-Text Search (FTS)**: Native CozoDB v0.7 FTS-Indizes mit **englischem** Stemming, Stopword-Filterung und robustem Query-Sanitizing (Bereinigung von `+ - * / \ ( ) ? .`) f√ºr maximale Stabilit√§t.
- **Near-Duplicate Detection (LSH)**: Erkennt automatisch sehr √§hnliche Beobachtungen via MinHash-LSH (CozoDB v0.7), um Redundanz zu vermeiden.
- **Recency Bias**: √§ltere Inhalte werden in der Fusion ged√§mpft (au√üer bei expliziter Keyword-Suche), damit ‚Äûaktuell relevant‚Äú h√§ufiger oben landet.
- **Graph-RAG & Graph-Walking (v1.7 Optimized)**: Erweitertes Retrieval-Verfahren, das semantische Vektor-Seeds mit rekursiven Graph-Traversals kombiniert. Nutzt nun einen optimierten **Graph-Walking** Algorithmus via Datalog, der HNSW-Index-Lookups f√ºr pr√§zise Distanzberechnungen w√§hrend der Traversierung verwendet.
- **Multi-Vector Support (v1.7)**: Jede Entit√§t verf√ºgt nun √ºber zwei spezialisierte Vektoren:
  1. **Content-Embedding**: Repr√§sentiert den inhaltlichen Kontext (Beobachtungen).
  2. **Name-Embedding**: Optimiert f√ºr die Identifikation via Namen/Label.
  Dies verbessert die Genauigkeit beim Einstieg in Graph-Walks signifikant.
- **Semantic & Persistent Caching (v0.8.5)**: Zweistufiges Caching-System:
  1. **L1 Memory Cache**: Ultraschneller In-Memory LRU-Cache (< 0.1ms).
  2. **L2 Persistent Cache**: Speicherung in CozoDB f√ºr Neustart-Resistenz.
  3. **Semantic Matching**: Erkennt semantisch √§hnliche Queries via Vektor-Distanz.
  4. **Janitor TTL**: Automatische Bereinigung veralteter Cache-Eintr√§ge durch den Janitor-Service.
- **Time-Travel**: √Ñnderungen werden √ºber CozoDB `Validity` versioniert; historische Abfragen sind m√∂glich.
- **JSON Merge Operator (++)**: Nutzt den v0.7 Merge-Operator f√ºr effiziente, atomare Metadaten-Updates.
- **Multi-Statement Transactions (v1.2)**: Unterst√ºtzt atomare Transaktionen √ºber mehrere Operationen hinweg mittels CozoDB-Block-Syntax `{ ... }`. Dies garantiert, dass zusammenh√§ngende √Ñnderungen (z.B. Entity erstellen + Observation hinzuf√ºgen + Beziehung kn√ºpfen) entweder vollst√§ndig oder gar nicht ausgef√ºhrt werden.
- **Graph-Metriken & Ranking Boost (v1.3 / v1.6)**: Integriert fortgeschrittene Graph-Algorithmen:
  - **PageRank**: Berechnet die "Wichtigkeit" von Wissensknoten f√ºr das Ranking.
  - **Betweenness Centrality**: Identifiziert zentrale Br√ºckenelemente im Wissensnetzwerk.
  - **HITS (Hubs & Authorities)**: Unterscheidet zwischen Informationsquellen (Authorities) und Wegweisern (Hubs).
  - **Connected Components**: Erkennt isolierte Wissensinseln und Teilgraphen.
  - Diese Metriken werden automatisch in der Hybrid-Suche (`advancedSearch`) und im `graphRag` genutzt.
- **Native CozoDB Operatoren (v1.5)**: Verwendet nun explizite `:insert`, `:update` und `:delete` Operatoren anstelle von generischen `:put` (upsert) Aufrufen. Dies erh√∂ht die Datensicherheit durch strikte Validierung der Datenbankzust√§nde (z. B. Fehler beim Versuch, eine existierende Entit√§t erneut zu "inserten").
- **Advanced Time-Travel Analysis (v1.5)**: Erweiterung der Beziehungs-Historie um Zeitbereichs-Filter (`since`/`until`) und automatische Diff-Zusammenfassungen, um Ver√§nderungen (Hinzuf√ºgungen/Entfernungen) √ºber spezifische Zeitr√§ume hinweg zu analysieren.
- **Graph-Features (v1.6)**: Native Integration von Shortest Path (Dijkstra) mit Pfad-Rekonstruktion, Community Detection (LabelPropagation) und fortgeschrittenen Zentralit√§tsma√üen.
- **Graph-Evolution**: Trackt die zeitliche Entwicklung von Beziehungen (z. B. Rollenwechsel von ‚ÄûManager‚Äú zu ‚ÄûBerater‚Äú) via CozoDB `Validity` Queries.
- **Bridge Discovery**: Identifiziert ‚ÄûBr√ºcken-Entit√§ten‚Äú, die verschiedene Communities verbinden ‚Äì ideal f√ºr kreatives Brainstorming.
- **Inference**: implizite Vorschl√§ge und Kontext-Erweiterung (z. B. transitive Expertise-Regel).
- **Konflikterkennung (Application-Level & Triggers)**: Erkennt automatisch Widerspr√ºche in den Metadaten (z. B. ‚Äûaktiv‚Äú vs. ‚Äûeingestellt‚Äú / `archived: true`). Nutzt eine robuste Logik in der App-Schicht, um Datenintegrit√§t vor dem Schreiben sicherzustellen.
- **Datenintegrit√§t (Trigger-Konzept)**: Verhindert ung√ºltige Zust√§nde wie Selbst-Referenzen in Beziehungen (Self-Loops) direkt bei der Erstellung.
- **Hierarchische Summarization**: Der Janitor verdichtet alte Fragmente zu ‚ÄûExecutive Summary‚Äú-Knoten, um das ‚ÄûBig Picture‚Äú langfristig zu erhalten.
- **User Preference Profiling**: Eine spezialisierte `global_user_profile` Entit√§t speichert persistente Pr√§ferenzen (Vorlieben, Arbeitsstil), die bei jeder Suche einen **50% Score-Boost** erhalten.
- **Alles lokal**: Embeddings via Transformers/ONNX; kein externer Embedding-Dienst n√∂tig.

## Positionierung & Vergleich

Die meisten "Memory"-MCP-Server lassen sich in zwei Kategorien einteilen:
1.  **Simple Knowledge-Graphs**: CRUD-Operationen auf Tripeln, oft nur Textsuche.
2.  **Reine Vector-Stores**: Semantische Suche (RAG), aber wenig Verst√§ndnis f√ºr komplexe Beziehungen.

Dieser Server f√ºllt die L√ºcke dazwischen ("Sweet Spot"): Eine **lokale, datenbankgest√ºtzte Memory-Engine**, die Vektor-, Graph- und Keyword-Signale kombiniert.

### Vergleich mit anderen L√∂sungen

| Feature | **CozoDB Memory (Dieses Projekt)** | **Official Reference (`@modelcontextprotocol/server-memory`)** | **mcp-memory-service (Community)** | **Datenbank-Adapter (Qdrant/Neo4j)** |
| :--- | :--- | :--- | :--- | :--- |
| **Backend** | **CozoDB** (Graph + Vektor + Relational) | JSON-Datei (`memory.jsonl`) | SQLite / Cloudflare | Spezialisierte DB (nur Vektor o. Graph) |
| **Such-Logik** | **Hybrid (RRF)**: Vektor + Keyword + Graph | Nur Keyword / Exakter Graph-Match | Vektor + Keyword | Meist nur eine Dimension |
| **Inference** | **Ja**: Eingebaute Engine f√ºr implizites Wissen | Nein | Nein ("Dreaming" ist Konsolidierung) | Nein (nur Retrieval) |
| **Time-Travel** | **Ja**: Abfragen zu jedem Zeitpunkt (`Validity`) | Nein (nur aktueller Stand) | Historie vorhanden, kein natives DB-Feature | Nein |
| **Wartung** | **Janitor**: LLM-gest√ºtzte Bereinigung | Manuell | Automatische Konsolidierung | Meist manuell |
| **Deployment** | **Lokal** (Node.js + Embedded DB) | Lokal (Docker/NPX) | Lokal oder Cloud | Ben√∂tigt oft externen DB-Server |

Der Kernvorteil ist **Retrieval-Qualit√§t und Nachvollziehbarkeit**: Durch die Kombination von Graph-Algorithmen (PageRank, Community Detection) und Vektor-Indizes (HNSW) kann Kontext viel pr√§ziser bereitgestellt werden als durch reine √Ñhnlichkeitssuche.

## Performance & Benchmarks

Benchmarks auf einem Standard-Entwickler-Laptop (Windows, Node.js 20+, nur CPU):

| Metrik | Wert | Anmerkung |
| :--- | :--- | :--- |
| **Graph-Walking (Rekursiv)** | **~130 ms** | Vektor-Seed + Rekursive Datalog-Traversierung |
| **Graph-RAG (Breadth-First)** | **~335 ms** | Vektor-Seeds + 2-Hop Expansion |
| **Hybrid Search (Cache Hit)** | **< 0.1 ms** | **v0.8+ Semantic Cache** |
| **Hybrid Search (Kalt)** | **~35 ms** | FTS + HNSW + RRF-Fusion |
| **Vektor-Suche (Raw)** | **~51 ms** | Reine semantische Suche als Referenz |
| **FTS-Suche (Raw)** | **~12 ms** | Native Full-Text Search Performance |
| **Ingestion** | **~102 ms** | Pro Op (Schreiben + Embedding + FTS/LSH Indexing) |
| **RAM-Verbrauch** | **~1.7 GB** | Prim√§r durch lokales `Xenova/bge-m3` modell |

### Benchmarks ausf√ºhren

Du kannst die Performance auf deinem System mit dem integrierten Benchmark-Tool testen:

```bash
npm run benchmark
```

Dieses Tool (`src/benchmark.ts`) f√ºhrt folgende Tests durch:
1.  **Initialisierung**: Kaltstart-Dauer des Servers inkl. Modell-Loading.
2.  **Ingestion**: Massen-Import von Test-Entit√§ten und Beobachtungen (Durchsatz).
3.  **Search Performance**: Latenz-Messung f√ºr Hybrid Search vs. Raw Vector Search.
4.  **RRF Overhead**: Ermittlung der zus√§tzlichen Rechenzeit f√ºr die Fusion-Logik.

## Architektur (high level)

```mermaid
graph TB
    Client[MCP Client<br/>Claude Desktop, etc.]
    Server[MCP Server<br/>FastMCP + Zod Schemas]
    Services[Memory Services]
    Embeddings[Embeddings<br/>ONNX Runtime]
    Search[Hybrid Search<br/>RRF Fusion]
    Cache[Semantic Cache<br/>L1 + L2]
    Inference[Inference Engine<br/>Multi-Strategy]
    DB[(CozoDB SQLite<br/>Relations + Validity<br/>HNSW Indizes<br/>Datalog/Graph)]
    
    Client -->|stdio| Server
    Server --> Services
    Services --> Embeddings
    Services --> Search
    Services --> Cache
    Services --> Inference
    Services --> DB
    
    style Client fill:#e1f5ff
    style Server fill:#fff4e1
    style Services fill:#f0e1ff
    style DB fill:#e1ffe1
```

### Graph-Walking Visualisierung

```mermaid
graph LR
    Start([Query: Woran arbeitet Alice?])
    V1[Vektor-Suche<br/>Finde: Alice]
    E1[Alice<br/>Person]
    E2[Projekt X<br/>Projekt]
    E3[Feature Flags<br/>Technologie]
    E4[Bob<br/>Person]
    
    Start --> V1
    V1 -.semantische √Ñhnlichkeit.-> E1
    E1 -->|works_on| E2
    E2 -->|uses_tech| E3
    E1 -->|colleague_of| E4
    E4 -.semantisch: auch relevant.-> E2
    
    style Start fill:#e1f5ff
    style V1 fill:#fff4e1
    style E1 fill:#ffe1e1
    style E2 fill:#e1ffe1
    style E3 fill:#f0e1ff
    style E4 fill:#ffe1e1
```

## Installation

### Voraussetzungen
- Node.js 20+ (empfohlen)
- Die CozoDB Native Dependency wird via `cozo-node` installiert.

### Setup

```bash
npm install
npm run build
```

### Windows Quickstart

```bash
npm install
npm run build
npm run start
```

Hinweise:
- Beim ersten Start l√§dt `@xenova/transformers` das Embedding-Modell (kann dauern).
- Die Embeddings werden auf der CPU verarbeitet.

## Start / Integration

### MCP Server (stdio)

Der MCP Server l√§uft √ºber stdio (f√ºr Claude Desktop & Co.). Start:

```bash
npm run start
```

Standard-Datenbankpfad: `memory_db.cozo.db` im Projektroot (wird automatisch angelegt).

### Claude Desktop Integration

```json
{
  "mcpServers": {
    "cozo-memory": {
      "command": "node",
      "args": ["C:/Pfad/zu/cozo-memory/dist/index.js"]
    }
  }
}
```

## Konfiguration & Backends

Das System unterst√ºtzt verschiedene Speicher-Backends. Standardm√§√üig wird **SQLite** verwendet, da es keine zus√§tzliche Installation erfordert und f√ºr die meisten Anwendungsf√§lle die beste Balance aus Performance und Einfachheit bietet.

### Backend wechseln (z. B. zu RocksDB)

RocksDB bietet Vorteile bei sehr gro√üen Datens√§tzen (Millionen von Eintr√§gen) und schreibintensiven Workloads durch bessere Parallelit√§t und Datenkompression.

Um das Backend zu wechseln, setzen Sie die Umgebungsvariable `DB_ENGINE` vor dem Start:

**PowerShell:**
```powershell
$env:DB_ENGINE="rocksdb"; npm run dev
```

**Bash:**
```bash
DB_ENGINE=rocksdb npm run dev
```

| Backend | Status | Empfehlung |
| :--- | :--- | :--- |
| **SQLite** | Aktiv (Standard) | Standard f√ºr Desktop/lokale Nutzung. |
| **RocksDB** | Vorbereitet & Getestet | F√ºr High-Performance oder sehr gro√üe Datenmengen. |
| **MDBX** | Nicht unterst√ºtzt | Erfordert manuellen Build von `cozo-node` aus dem Quellcode. |

---

## Datenmodell

CozoDB-Relations (vereinfacht) ‚Äì alle Schreiboperationen erzeugen neue `Validity`-Eintr√§ge (Time-Travel):
- `entity`: `id`, `created_at: Validity` ‚áí `name`, `type`, `embedding(1024)`, `name_embedding(1024)`, `metadata(Json)`
- `observation`: `id`, `created_at: Validity` ‚áí `entity_id`, `text`, `embedding(1024)`, `metadata(Json)`
- `relationship`: `from_id`, `to_id`, `relation_type`, `created_at: Validity` ‚áí `strength(0..1)`, `metadata(Json)`
- `entity_community`: `entity_id` ‚áí `community_id` (Key-Value Mapping aus LabelPropagation)
- `memory_snapshot`: `snapshot_id` ‚áí Counts + `metadata` + `created_at(Int)`

## MCP Tools

Die Oberfl√§che ist auf **4 konsolidierte Tools** reduziert. Die konkrete Operation wird immer √ºber `action` gew√§hlt.

| Tool | Zweck | Wichtige Aktionen |
|------|-------|-------------------|
| `mutate_memory` | Schreiboperationen | create_entity, update_entity, delete_entity, add_observation, create_relation, run_transaction, add_inference_rule, ingest_file |
| `query_memory` | Leseoperationen | search, advancedSearch, context, entity_details, history, graph_rag, graph_walking |
| `analyze_graph` | Graph-Analyse | explore, communities, pagerank, betweenness, hits, shortest_path, bridge_discovery, semantic_walk, infer_relations |
| `manage_system` | Wartung | health, metrics, export_memory, import_memory, snapshot_create, snapshot_list, snapshot_diff, cleanup, reflect, clear_memory |

### mutate_memory (Schreiben)

Aktionen:
- `create_entity`: `{ name, type, metadata? }`
- `update_entity`: `{ id, name?, type?, metadata? }`
- `delete_entity`: `{ entity_id }`
- `add_observation`: `{ entity_id?, entity_name?, entity_type?, text, metadata? }`
- `create_relation`: `{ from_id, to_id, relation_type, strength?, metadata? }`
- `run_transaction`: `{ operations: Array<{ action, params }> }` **(Neu v1.2)**: F√ºhrt mehrere Operationen atomar aus.
- `add_inference_rule`: `{ name, datalog }`
- `ingest_file`: `{ format, file_path?, content?, entity_id?, entity_name?, entity_type?, chunking?, metadata?, observation_metadata?, deduplicate?, max_observations? }`
  - `format` Optionen: `"markdown"`, `"json"`, `"pdf"` **(Neu v1.9)**
  - `file_path`: Optionaler Pfad zur Datei auf der Festplatte (Alternative zum `content` Parameter)
  - `content`: Dateiinhalt als String (erforderlich wenn `file_path` nicht angegeben)
  - `chunking` Optionen: `"none"`, `"paragraphs"` (zuk√ºnftig: `"semantic"`)

Wichtige Details:
- `run_transaction` unterst√ºtzt `create_entity`, `add_observation` und `create_relation`. Parameter werden automatisch suffigiert, um Kollisionen zu vermeiden.
- `create_relation` lehnt Selbst-Referenzen (`from_id === to_id`) ab.
- `strength` ist optional und defaultet auf `1.0`.
- `add_observation` liefert zus√§tzlich `inferred_suggestions` (Vorschl√§ge aus der Inference Engine).
- `add_observation` macht Deduplication (exakt + semantisch via LSH). Bei Duplikaten kommt `status: "duplicate_detected"` mit `existing_observation_id` und einer gesch√§tzten `similarity`.
- `update_entity` nutzt den JSON-Merge Operator `++` (v0.7), um bestehende Metadaten mit den neuen Werten zu vereinen, anstatt sie zu √ºberschreiben.
- `add_inference_rule` validiert den Datalog-Code beim Speichern. Ung√ºltige Syntax oder fehlende Pflichtspalten f√ºhren zu einem Fehler.

Beispiele:

```json
{ "action": "create_entity", "name": "Alice", "type": "Person", "metadata": { "role": "Dev" } }
```

```json
{ "action": "add_observation", "entity_id": "ENTITY_ID", "text": "Alice arbeitet am Feature-Flag-System." }
```

Beispiel (Duplikat):

```json
{ "action": "add_observation", "entity_id": "ENTITY_ID", "text": "Alice arbeitet am Feature-Flag-System." }
```

```json
{ "status": "duplicate_detected", "existing_observation_id": "OBS_ID", "similarity": 1 }
```

```json
{ "action": "create_relation", "from_id": "ALICE_ID", "to_id": "PROJ_ID", "relation_type": "works_on", "strength": 1.0 }
```

Custom Datalog Regeln (Inference):

- Inferenzregeln werden als `action: "add_inference_rule"` gespeichert.
- Die Datalog-Query muss ein Resultset mit **genau diesen 5 Spalten** liefern: `from_id, to_id, relation_type, confidence, reason`.
- `$id` ist der Platzhalter f√ºr die Entity-ID, f√ºr die die Inferenz gestartet wird.
- Tabellen im Schema:
    - `*entity{id, name, type, metadata, @ "NOW"}`
    - `*relationship{from_id, to_id, relation_type, strength, metadata, @ "NOW"}`
    - `*observation{id, entity_id, text, metadata, @ "NOW"}`

Beispiel (Transitiver Manager ‚áí Ober-Manager):

```json
{
  "action": "add_inference_rule",
  "name": "ober_manager",
  "datalog": "?[from_id, to_id, relation_type, confidence, reason] :=\n  *relationship{from_id: $id, to_id: mid, relation_type: \"manager_of\", @ \"NOW\"},\n  *relationship{from_id: mid, to_id: target, relation_type: \"manager_of\", @ \"NOW\"},\n  from_id = $id,\n  to_id = target,\n  relation_type = \"ober_manager_von\",\n  confidence = 0.6,\n  reason = \"Transitiver Manager-Pfad gefunden\""
}
```

Beispiel (Reziproke Beziehung vorschlagen):

```json
{
  "action": "add_inference_rule",
  "name": "colleague_symmetry",
  "datalog": "?[from_id, to_id, relation_type, confidence, reason] :=\n  *relationship{from_id: other, to_id: $id, relation_type: \"colleague_of\", @ \"NOW\"},\n  from_id = $id,\n  to_id = other,\n  relation_type = \"colleague_of\",\n  confidence = 0.9,\n  reason = \"Symmetrische Kollegenbeziehung\""
}
```

Regeln testen (liefert Vorschl√§ge, persistiert sie aber nicht automatisch):

```json
{ "action": "infer_relations", "entity_id": "ENTITY_ID" }
```

Bulk-Ingestion (Markdown/JSON/PDF):

```json
{
  "action": "ingest_file",
  "entity_name": "Projektdokumentation",
  "format": "markdown",
  "chunking": "paragraphs",
  "content": "# Titel\n\nAbschnitt 1...\n\nAbschnitt 2...",
  "deduplicate": true,
  "max_observations": 50
}
```

PDF-Ingestion √ºber Dateipfad:

```json
{
  "action": "ingest_file",
  "entity_name": "Forschungsarbeit",
  "format": "pdf",
  "file_path": "/pfad/zur/dokument.pdf",
  "chunking": "paragraphs",
  "deduplicate": true
}
```

### query_memory (Lesen)

Aktionen:
- `search`: `{ query, limit?, entity_types?, include_entities?, include_observations? }`
- `advancedSearch`: `{ query, limit?, filters?, graphConstraints?, vectorOptions? }` **(Neu v1.1 / v1.4)**: Erweiterte Suche mit nativen HNSW-Filtern (Typen) und robustem Post-Filtering (Metadaten, Zeit).
- `context`: `{ query, context_window?, time_range_hours? }`
- `entity_details`: `{ entity_id, as_of? }`
- `history`: `{ entity_id }`
- `graph_rag`: `{ query, max_depth?, limit?, filters? }` Graph-basiertes Reasoning. Findet erst Vektor-Seeds (mit Inline-Filtering) und expandiert dann transitive Beziehungen. Nutzt rekursives Datalog f√ºr effiziente BFS-Expansion.
- `graph_walking`: `{ query, start_entity_id?, max_depth?, limit? }` (v1.7) Rekursive semantische Graph-Suche. Startet bei Vektor-Seeds oder einer spezifischen Entit√§t und folgt Beziehungen zu anderen semantisch relevanten Entit√§ten. Ideal f√ºr tiefere Pfad-Exploration.
- `get_relation_evolution`: `{ from_id, to_id?, since?, until? }` (in `analyze_graph`) Zeigt die zeitliche Entwicklung von Beziehungen inklusive Zeitbereichs-Filter und Diff-Zusammenfassung.

Wichtige Details:
- `advancedSearch` erlaubt pr√§zise Filterung:
    - `filters.entityTypes`: Liste von Entit√§tstypen.
    - `filters.metadata`: Key-Value Map f√ºr exakte Metadaten-Treffer.
    - `graphConstraints.requiredRelations`: Nur Entit√§ten, die bestimmte Beziehungen haben.
    - `graphConstraints.targetEntityIds`: Nur Entit√§ten, die mit diesen Ziel-IDs verbunden sind.
- `context` liefert ein JSON-Objekt mit Entities, Observations, Graph-Verbindungen und Inferenz-Vorschl√§gen.
- `search` nutzt RRF (Reciprocal Rank Fusion), um Vektor- und Keyword-Signale zu mischen.
- `graph_rag` kombiniert Vektor-Suche mit graph-basierten Traversals (Standard-Tiefe 2) f√ºr "strukturiertes Reasoning". Die Expansion erfolgt bidirektional √ºber alle Beziehungstypen.
- **User Profiling**: Ergebnisse, die mit der `global_user_profile` Entit√§t verkn√ºpft sind, werden automatisch bevorzugt (Boost).
- `time_range_hours` filtert die Ergebnis-Kandidaten im Zeitfenster (in Stunden, kann float sein).
- `as_of` akzeptiert ISO-Strings oder `"NOW"`; invalides Format f√ºhrt zu einem Fehler.
- Bei erkannten Status-Widerspr√ºchen wird optional `conflict_flag` an Entities/Observations angeh√§ngt; `context` liefert zus√§tzlich `conflict_flags` als Zusammenfassung.

Beispiele:

```json
{ "action": "search", "query": "Feature Flag", "limit": 10 }
```

```json
{ 
  "action": "advancedSearch", 
  "query": "Manager", 
  "filters": { "metadata": { "role": "Lead" } },
  "graphConstraints": { "requiredRelations": ["works_with"] }
}
```

```json
{ "action": "graph_rag", "query": "Woran arbeitet Alice?", "max_depth": 2 }
```

```json
{ "action": "context", "query": "Woran arbeitet Alice gerade?", "context_window": 20 }
```

```json
{ "action": "context", "query": "Woran arbeitet Alice gerade?", "context_window": 20, "time_range_hours": 24 }
```

```json
{ "action": "entity_details", "entity_id": "ENTITY_ID", "as_of": "2026-02-01T12:00:00Z" }
```

#### Konflikterkennung (Status)

Wenn es f√ºr eine Entity widerspr√ºchliche Aussagen zum Status gibt, wird ein Konflikt markiert. Dabei ber√ºcksichtigt das System **temporale Konsistenz**:

- **Status-Widerspruch**: Eine Entity hat im **gleichen Kalenderjahr** sowohl den Status ‚Äûaktiv‚Äú als auch ‚Äûinaktiv‚Äú.
- **Status-√Ñnderung (Kein Konflikt)**: Wenn die Aussagen aus unterschiedlichen Jahren stammen (z.B. 2024 ‚Äûeingestellt‚Äú, 2025 ‚Äûaktiv‚Äú), wird dies als legitime √Ñnderung interpretiert und **nicht** als Konflikt markiert.

Die Erkennung nutzt Regex-Matching auf Keywords wie:
- **Aktiv**: aktiv, l√§uft, ongoing, active, running, in betrieb, fortgesetzt, weiter gef√ºhrt, nicht eingestellt.
- **Inaktiv**: eingestellt, abgebrochen, gestoppt, stillgelegt, geschlossen, shutdown, deprecated, archiviert, beendet, aufgegeben.

**Integration in API-Antworten:**
- `entities[i].conflict_flag` bzw. `observations[i].conflict_flag`: Flag direkt am Treffer.
- `conflict_flags`: Liste aller erkannten Konflikte im `context`- oder `search`-Result.

Beispiel f√ºr einen erkannten Konflikt (gleiches Jahr):

```json
{
  "entities": [
    {
      "id": "‚Ä¶",
      "name": "Projekt X",
      "type": "Project",
      "conflict_flag": {
        "entity_id": "‚Ä¶",
        "entity_name": "Projekt X",
        "entity_type": "Project",
        "kind": "status",
        "summary": "Konflikt: Es gibt widerspr√ºchliche Infos zum Status von Projekt X im gleichen Zeitraum (2026).",
        "evidence": {
          "active": { "created_at": 1767225600000000, "year": 2026, "text": "Projekt X ist aktiv." },
          "inactive": { "created_at": 1769904000000000, "year": 2026, "text": "Projekt X ist gestoppt." }
        }
      }
    }
  ]
}
```

### analyze_graph (Analyse)

Aktionen:
- `explore`: `{ start_entity, end_entity?, max_hops?, relation_types? }`
    - mit `end_entity`: k√ºrzester Pfad (BFS)
    - ohne `end_entity`: Nachbarschaft bis max. 5 Hops (aggregiert nach minimaler Hop-Anzahl)
- `communities`: `{}` berechnet Communities neu und schreibt `entity_community`
- `pagerank`: `{}` Berechnet PageRank-Scores f√ºr alle Entit√§ten.
- `betweenness`: `{}` Berechnet Betweenness Centrality (Zentralit√§tsma√ü f√ºr Br√ºckenelemente).
- `hits`: `{}` Berechnet HITS-Scores (Hubs & Authorities).
- `connected_components`: `{}` Identifiziert isolierte Teilgraphen.
- `shortest_path`: `{ start_entity, end_entity }` Berechnet den k√ºrzesten Pfad via Dijkstra (inkl. Distanz und Pfad-Rekonstruktion).
- `bridge_discovery`: `{}` Sucht nach Entit√§ten, die als Br√ºcken zwischen isolierten Communities fungieren (hohe Betweenness-Relevanz)
- `semantic_walk`: `{ start_entity, max_depth?, min_similarity? }` (v1.7) Rekursive semantische Graph-Suche. Startet bei einer Entit√§t und folgt rekursiv Pfaden, die aus expliziten Beziehungen UND semantischer √Ñhnlichkeit (Vektor-Distanz) bestehen. Findet "assoziative Pfade" im Wissensgraphen.
- `hnsw_clusters`: `{}` Analysiert Cluster direkt auf dem HNSW-Graphen (Layer 0). Extrem schnell, da keine Vektorberechnungen n√∂tig sind.
- `infer_relations`: `{ entity_id }` liefert Vorschl√§ge aus mehreren Strategien
- `get_relation_evolution`: `{ from_id, to_id?, since?, until? }` zeigt die zeitliche Entwicklung von Beziehungen inklusive Zeitbereichs-Filter und Diff-Zusammenfassung.

Beispiele:

```json
{ "action": "shortest_path", "start_entity": "ID_A", "end_entity": "ID_B" }
```

```json
{ "action": "hits" }
```

```json
{ "action": "explore", "start_entity": "ENTITY_ID", "max_hops": 3 }
```

```json
{ "action": "get_relation_evolution", "from_id": "ALICE_ID", "to_id": "PROJECT_X_ID" }
```

```json
{ "action": "infer_relations", "entity_id": "ENTITY_ID" }
```

### manage_system (Wartung)

Aktionen:
- `health`: `{}` liefert DB-Counts + Embedding-Cache-Stats + Performance-Metriken.
- `metrics`: `{}` liefert detaillierte Operationsz√§hler, Fehlerstatistiken und Performance-Daten.
- `export_memory`: `{ format, includeMetadata?, includeRelationships?, includeObservations?, entityTypes?, since? }` exportiert Memory in verschiedene Formate.
- `import_memory`: `{ data, sourceFormat, mergeStrategy?, defaultEntityType? }` importiert Memory aus externen Quellen.
- `snapshot_create`: `{ metadata? }`
- `snapshot_list`: `{}`
- `snapshot_diff`: `{ snapshot_id_a, snapshot_id_b }`
- `cleanup`: `{ confirm, older_than_days?, max_observations?, min_entity_degree?, model? }`
- `reflect`: `{ entity_id?, model? }` Analysiert Memory auf Widerspr√ºche und neue Einsichten.
- `clear_memory`: `{ confirm }`

Janitor-Cleanup Details:
- `cleanup` unterst√ºtzt `dry_run`: bei `confirm: false` werden nur Kandidaten gelistet.
- Bei `confirm: true` wird der Janitor aktiv:
  - **Hierarchische Summarization**: Erkennt isolierte oder alte Beobachtungen, l√§sst sie von einer lokalen LLM (Ollama) zusammenfassen und erstellt einen neuen `ExecutiveSummary`-Knoten. Die alten Fragmente werden gel√∂scht, um Rauschen zu reduzieren, w√§hrend das Wissen erhalten bleibt.

**Vor Janitor:**
```
Entity: Projekt X
‚îú‚îÄ Observation 1: "Gestartet in Q1" (90 Tage alt, isoliert)
‚îú‚îÄ Observation 2: "Nutzt React" (85 Tage alt, isoliert)
‚îú‚îÄ Observation 3: "Team von 5" (80 Tage alt, isoliert)
‚îî‚îÄ Observation 4: "Deployed auf Staging" (75 Tage alt, isoliert)
```

**Nach Janitor:**
```
Entity: Projekt X
‚îî‚îÄ ExecutiveSummary: "Projekt X ist eine React-basierte Anwendung, die in Q1 
   mit einem Team von 5 Entwicklern gestartet wurde und aktuell auf der 
   Staging-Umgebung deployed ist."
```

Reflexions-Service Details:
- `reflect` analysiert Beobachtungen einer Entit√§t (oder der Top 5 aktivsten Entit√§ten), um Widerspr√ºche, Muster oder zeitliche Entwicklungen zu finden.
- Ergebnisse werden als neue Beobachtungen mit dem Metadaten-Feld `{ "kind": "reflection" }` persistiert und sind √ºber `context` abrufbar.
- Der Text wird mit dem Pr√§fix `Reflexive Einsicht: ` gespeichert.

Defaults: `older_than_days=30`, `max_observations=20`, `min_entity_degree=2`, `model="demyagent-4b-i1:Q6_K"`.

Export/Import Details:
- `export_memory` unterst√ºtzt drei Formate:
  - **JSON** (`format: "json"`): Natives Cozo-Format, vollst√§ndig re-importierbar mit allen Metadaten und Zeitstempeln.
  - **Markdown** (`format: "markdown"`): Menschenlesbares Dokument mit Entities, Observations und Relationships.
  - **Obsidian** (`format: "obsidian"`): ZIP-Archiv mit Wiki-Links `[[Entity]]`, YAML-Frontmatter, bereit f√ºr Obsidian-Vault.
- `import_memory` unterst√ºtzt vier Quellformate:
  - **Cozo** (`sourceFormat: "cozo"`): Import aus nativem JSON-Export.
  - **Mem0** (`sourceFormat: "mem0"`): Import aus Mem0-Format (user_id wird zu Entity).
  - **MemGPT** (`sourceFormat: "memgpt"`): Import aus MemGPT Archival/Recall Memory.
  - **Markdown** (`sourceFormat: "markdown"`): Parse Markdown-Abschnitte als Entities mit Observations.
- Merge-Strategien: `skip` (Standard, Duplikate √ºberspringen), `overwrite` (Existierende ersetzen), `merge` (Metadaten kombinieren).
- Optionale Filter: `entityTypes` (Array), `since` (Unix-Timestamp in ms), `includeMetadata`, `includeRelationships`, `includeObservations`.

Beispiel Export:
```json
{
  "action": "export_memory",
  "format": "obsidian",
  "includeMetadata": true,
  "entityTypes": ["Person", "Project"]
}
```

Beispiel Import:
```json
{
  "action": "import_memory",
  "sourceFormat": "mem0",
  "data": "{\"user_id\": \"alice\", \"memories\": [...]}",
  "mergeStrategy": "skip"
}
```

Production Monitoring Details:
- `health` liefert umfassenden Systemstatus inklusive Entity/Observation/Relationship-Counts, Embedding-Cache-Statistiken und Performance-Metriken (letzte Operationszeit, durchschnittliche Operationszeit, Gesamtanzahl Operationen).
- `metrics` liefert detaillierte Betriebsmetriken:
  - **Operationsz√§hler**: Trackt create_entity, update_entity, delete_entity, add_observation, create_relation, search und graph_operations.
  - **Fehlerstatistiken**: Gesamtfehler und Aufschl√ºsselung nach Operationstyp.
  - **Performance-Metriken**: Dauer der letzten Operation, durchschnittliche Operationsdauer und Gesamtanzahl ausgef√ºhrter Operationen.
- Delete-Operationen enthalten nun detailliertes Logging mit Verifikationsschritten und liefern Statistiken √ºber gel√∂schte Daten (Observations, ausgehende/eingehende Relationen).

Beispiele:

```json
{ "action": "health" }
```

```json
{ "action": "reflect", "entity_id": "ENTITY_ID" }
```

```json
{ "action": "cleanup", "confirm": false, "older_than_days": 60, "max_observations": 25 }
```

```json
{ "action": "snapshot_diff", "snapshot_id_a": "SNAP_A", "snapshot_id_b": "SNAP_B" }
```

```json
{ "action": "clear_memory", "confirm": true }
```

## Production Monitoring

Das System enth√§lt umfassende Monitoring-Funktionen f√ºr Produktionsumgebungen:

### Metriken-Tracking

Alle Operationen werden automatisch mit detaillierten Metriken erfasst:
- Operationsz√§hler nach Typ (create, update, delete, search, etc.)
- Fehler-Tracking mit Aufschl√ºsselung nach Operation
- Performance-Metriken (Latenz, Durchsatz)

### Health Endpoint

Die `health`-Aktion liefert Echtzeit-Systemstatus:
```json
{ "action": "health" }
```

Liefert:
- Datenbank-Counts (Entities, Observations, Relationships)
- Embedding-Cache-Statistiken (Hit-Rate, Gr√∂√üe)
- Performance-Metriken (letzte Operationszeit, Durchschnittszeit, Gesamtoperationen)

### Metrics Endpoint

Die `metrics`-Aktion liefert detaillierte Betriebsmetriken:
```json
{ "action": "metrics" }
```

Liefert:
- **operations**: Anzahl jedes Operationstyps
- **errors**: Gesamtfehler und Aufschl√ºsselung nach Operation
- **performance**: Dauer der letzten Operation, Durchschnittsdauer, Gesamtoperationen

### Erweiterte Delete-Operationen

Delete-Operationen enthalten umfassendes Logging und Verifikation:
- Detailliertes Schritt-f√ºr-Schritt-Logging mit `[Delete]`-Pr√§fix
- Z√§hlt zugeh√∂rige Daten vor dem L√∂schen
- Verifikation nach dem L√∂schen
- Liefert Statistiken: `{ deleted: { observations: N, outgoing_relations: N, incoming_relations: N } }`

Beispiel:
```json
{ "action": "delete_entity", "entity_id": "ENTITY_ID" }
```

Liefert L√∂schstatistiken, die genau zeigen, was entfernt wurde.

## Technische Highlights

### Duales Zeitstempel-Format (v1.9)

Alle Schreiboperationen (`create_entity`, `add_observation`, `create_relation`) geben Zeitstempel in beiden Formaten zur√ºck:
- `created_at`: Unix-Mikrosekunden (natives CozoDB-Format, pr√§zise f√ºr Berechnungen)
- `created_at_iso`: ISO 8601 String (menschenlesbar, z.B. `"2026-02-28T17:21:19.343Z"`)

Dieses duale Format bietet maximale Flexibilit√§t - verwende Unix-Zeitstempel f√ºr Zeitberechnungen und Vergleiche, oder ISO-Strings f√ºr Anzeige und Logging.

Beispiel-Antwort:
```json
{
  "id": "...",
  "created_at": 1772299279343000,
  "created_at_iso": "2026-02-28T17:21:19.343Z",
  "status": "Entity created"
}
```

### Local ONNX Embeddings (Transformers)

Default-Modell: `Xenova/bge-m3` (1024 Dimensionen).

Die Embeddings werden auf der CPU verarbeitet, um maximale Kompatibilit√§t zu gew√§hrleisten. Sie werden in einem LRU-Cache gehalten (1000 Eintr√§ge, 1h TTL). Bei Embedding-Fehlern wird ein Nullvektor zur√ºckgegeben, damit Tool-Aufrufe stabil bleiben.

### Hybrid Search (Vector + Keyword + Graph + Inference) + RRF

Die Suche kombiniert:
- Vektor-√Ñhnlichkeit √ºber HNSW Indizes (`~entity:semantic`, `~observation:semantic`)
- Keyword Matching via Regex (`regex_matches(...)`)
- Graph-Signal √ºber PageRank (f√ºr zentrale Entities)
- Community Expansion: Entities aus der Community der Top Seeds werden mit einem Boost eingebracht
- Inference-Signal: probabilistische Kandidaten (z. B. `expert_in`) mit `confidence` als Score

Fusion: Reciprocal Rank Fusion (RRF) √ºber Quellen `vector`, `keyword`, `graph`, `community`, `inference`.

Zeitlicher Decay (standardm√§√üig aktiv):
- Vor der RRF-Fusion werden Scores zeitbasiert ged√§mpft, basierend auf `created_at` (Validity).
- Halbwertszeit: 90 Tage (exponentieller Decay), mit Source-spezifischen Floors:
  - `keyword`: kein Decay (entspricht ‚Äûexplizit gesucht‚Äú)
  - `graph`/`community`: mindestens 0.6
  - `vector`: mindestens 0.2

Uncertainty/Transparenz:
- Inferenz-Kandidaten werden als `source: "inference"` markiert und liefern eine kurze Begr√ºndung (Unsch√§rfe-Hinweis) im Ergebnis.
- Im `context`-Output wird f√ºr inferierte Entities zus√§tzlich ein `uncertainty_hint` mitgeliefert, damit ein LLM ‚Äûharter Fakt‚Äú vs. ‚ÄûVermutung‚Äú unterscheiden kann.

### Inference Engine

Inference nutzt mehrere Strategien (nicht persistierend):
- **Co-occurrence**: Entit√§tsnamen in Observation-Texten (`related_to`, confidence 0.7)
- **Semantische N√§he**: √§hnliche Entities via HNSW (`similar_to`, bis max. 0.9)
- **Transitivit√§t**: A‚ÜíB und B‚ÜíC (`potentially_related`, confidence 0.5)
- **Expertise-Regel**: `Person` + `works_on` + `uses_tech` ‚áí `expert_in` (confidence 0.7)
- **Query-Triggered Expertise**: Bei Suchanfragen mit Keywords wie `expert`, `skill`, `knowledge`, `competence` etc. wird automatisch eine dedizierte Expertensuche √ºber das Graph-Netzwerk gestartet.

## Optional: HTTP API Bridge

### API Bridge

F√ºr UI/Tools gibt es einen Express-Server, der den `MemoryServer` direkt einbettet.

Start:

```bash
npm run bridge
```

Konfiguration:
- Port √ºber `PORT` (Default: `3001`)

Ausgew√§hlte Endpoints (Prefix `/api`):
- `GET /entities`, `POST /entities`, `GET /entities/:id`, `DELETE /entities/:id`
- `POST /observations`
- `GET /search`, `GET /context`
- `GET /health`
- `GET /snapshots`, `POST /snapshots`

## Entwicklung

### Struktur
- `src/index.ts`: MCP-Server + Tool-Registrierung + Schema Setup
- `src/embedding-service.ts`: Embedding Pipeline + LRU Cache
- `src/hybrid-search.ts`: Suchstrategien + RRF + Community Expansion
- `src/inference-engine.ts`: Inference Strategien
- `src/api_bridge.ts`: Express API Bridge (f√ºr UI)

### Scripts (Root)
- `npm run build`: TypeScript Build
- `npm run dev`: ts-node Start des MCP Servers
- `npm run start`: Startet `dist/index.js` (stdio)
- `npm run bridge`: Build + Start der API Bridge (`dist/api_bridge.js`)
- `npm run benchmark`: F√ºhrt Performance-Tests durch

## User Preference Profiling (Mem0-Style)

Das System pflegt ein persistentes Profil √ºber den Benutzer (Vorlieben, Abneigungen, Arbeitsstil) √ºber die spezialisierte Entit√§t `global_user_profile`.

- **Vorteil**: Personalisierung ohne manuelle Suche ("Ich wei√ü, dass du TypeScript bevorzugst").
- **Funktionsweise**: Alle Beobachtungen, die dieser Entit√§t zugeordnet werden, erhalten bei Such- und Kontext-Anfragen einen signifikanten Boost.
- **Initialisierung**: Das Profil wird beim ersten Start automatisch angelegt.

### Manuelle Tests

Es gibt verschiedene Test-Skripte f√ºr unterschiedliche Features:

```bash
# Testet Edge-Cases und Basis-Operationen
npx ts-node src/test-edge-cases.ts

# Testet Hybrid-Suche und Kontext-Retrieval
npx ts-node src/test-context.ts

# Testet die Memory-Reflexion (ben√∂tigt Ollama)
npx ts-node test-reflection.ts

# Testet das User Preference Profiling und den Search-Boost
npx ts-node test-user-pref.ts
```

## Troubleshooting

### H√§ufige Probleme

**Erster Start dauert lange**
- Der Embedding-Model-Download dauert beim ersten Start 30-90 Sekunden (Transformers l√§dt ~500MB Artefakte)
- Dies ist normal und passiert nur einmal
- Nachfolgende Starts sind schnell (< 2 Sekunden)

**Cleanup/Reflect ben√∂tigt Ollama**
- Bei Verwendung von `cleanup` oder `reflect` Aktionen muss ein Ollama-Dienst lokal laufen
- Ollama installieren von https://ollama.ai
- Gew√ºnschtes Modell pullen: `ollama pull demyagent-4b-i1:Q6_K` (oder bevorzugtes Modell)

**Windows-Spezifisch**
- Embeddings werden auf der CPU verarbeitet f√ºr maximale Kompatibilit√§t
- RocksDB-Backend ben√∂tigt Visual C++ Redistributable bei Verwendung dieser Option

**Performance-Probleme**
- Erste Query nach Neustart ist langsamer (kalter Cache)
- `health` Aktion nutzen um Cache-Hit-Raten zu pr√ºfen
- RocksDB-Backend erw√§gen f√ºr Datasets > 100k Entities

## Lizenz

Dieses Projekt ist unter der Apache-2.0 Lizenz lizenziert. Siehe die [LICENSE](LICENSE) Datei f√ºr Details.

## Haftungsausschluss

Single-User, Local-First: Dieses Projekt wurde entwickelt, um auf einem einzelnen Benutzer und einer lokalen Installation zu funktionieren.