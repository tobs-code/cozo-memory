# CozoDB Memory MCP Server (Archiv/Referenz)

[![npm](https://img.shields.io/npm/v/cozo-memory)](https://www.npmjs.com/package/cozo-memory)
[![Node](https://img.shields.io/node/v/cozo-memory)](https://nodejs.org)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue)](LICENSE)

> **Hinweis:** Dieses Repository wird prim√§r auf Englisch gepflegt. Die deutsche Dokumentation dient nur noch als Referenz und wird nicht mehr aktiv aktualisiert. Alle System-Komponenten (FTS, Keywords, Logs) sind nun auf Englisch optimiert.

**Local-first Memory f√ºr Claude & AI-Agenten mit Hybrid-Suche, Graph-RAG und Time-Travel ‚Äì alles in einer einzigen Binary, kein Cloud, kein Docker.**

## Inhaltsverzeichnis

- [Installation](#installation)
- [√úberblick](#√ºberblick)
- [Positionierung & Vergleich](#positionierung--vergleich)
- [Performance & Benchmarks](#performance--benchmarks)
- [Architektur](#architektur-high-level)
- [Start / Integration](#start--integration)
- [Konfiguration & Backends](#konfiguration--backends)
- [Datenmodell](#datenmodell)
- [MCP Tools](#mcp-tools)
  - [mutate_memory (Schreiben)](#mutate_memory-schreiben)
  - [query_memory (Lesen)](#query_memory-lesen)
  - [analyze_graph (Analyse)](#analyze_graph-analyse)
  - [manage_system (Wartung)](#manage_system-wartung)
- [Production Monitoring](#production-monitoring)
- [Technische Highlights](#technische-highlights)
- [Optional: HTTP API Bridge](#optional-http-api-bridge)
- [Entwicklung](#entwicklung)
- [User Preference Profiling](#user-preference-profiling-mem0-style)
- [Troubleshooting](#troubleshooting)
- [Lizenz](#lizenz)

## Installation

### Via npm (Empfohlen)

```bash
# Global installieren
npm install -g cozo-memory

# Oder direkt mit npx nutzen (keine Installation n√∂tig)
npx cozo-memory
```

### Aus dem Quellcode

```bash
git clone https://github.com/tobs-code/cozo-memory
cd cozo-memory
npm install && npm run build
npm run start
```

## √úberblick

üîç **Hybrid-Suche (seit v0.7)** - Kombination aus semantischer Suche (HNSW), Full-Text Search (FTS) und Graph-Signalen via Reciprocal Rank Fusion (RRF)

üîÄ **Dynamic Fusion Framework (v2.3)** - Fortgeschrittenes 4-Pfad-Retrieval-System, das Dense Vector, Sparse Vector, FTS und Graph-Traversal mit konfigurierbaren Gewichten und Fusion-Strategien (RRF, Weighted Sum, Max, Adaptive) kombiniert

‚è≥ **Temporal Graph Neural Networks (v2.4)** - Zeitbewusste Node-Embeddings, die historischen Kontext, zeitliche Gl√§tte und Recency-gewichtete Aggregation mittels Time2Vec-Encoding und Multi-Signal-Fusion erfassen

üîÄ **Multi-Hop Reasoning mit Vector Pivots (v2.5)** - Logik-bewusste Retrieve-Reason-Prune-Pipeline, die Vector Search als Sprungbrett f√ºr Graph-Traversierung mit Helpfulness-Scoring und Pivot-Depth-Security nutzt

üï∏Ô∏è **Graph-RAG & Graph-Walking (v1.7/v2.0)** - Hierarchisches Retrieval mit Community-Detection und Summarization; rekursive Traversals via optimierte Datalog-Algorithmen

üß† **Agentic Retrieval Layer (v2.0)** - Auto-Routing Engine, die den Query-Intent via lokalem LLM analysiert, um die optimale Suchstrategie (Vector, Graph oder Community) zu w√§hlen.

üß† **Multi-Level Memory (v2.0)** - Kontext-bewusstes Memory-System mit integriertem Session- und Task-Management.

üéØ **Tiny Learned Reranker (v2.0)** - Integriertes Cross-Encoder Modell (`ms-marco-MiniLM-L-6-v2`) f√ºr ultra-pr√§zises Re-Ranking der Top-Suchergebnisse.

üéØ **Multi-Vector Support (seit v1.7)** - Duale Embeddings pro Entity: Content-Embedding f√ºr Kontext, Name-Embedding f√ºr Identifikation

‚ö° **Semantic Caching (seit v0.8.5)** - Zweistufiger Cache (L1 Memory + L2 Persistent) mit semantischem Query-Matching

‚è±Ô∏è **Time-Travel-Abfragen** - Versionierung aller √Ñnderungen via CozoDB Validity; Abfragen zu jedem Zeitpunkt

üîó **Atomare Transaktionen (seit v1.2)** - Multi-Statement-Transaktionen f√ºr Datenkonsistenz

üìä **Graph-Algorithmen (seit v1.3/v1.6)** - PageRank, Betweenness Centrality, HITS, Community Detection, Shortest Path

üèóÔ∏è **Hierarchical GraphRAG (v2.0)** - Automatische Generierung thematischer "Community Summaries" mittels lokaler LLMs f√ºr globales "Big Picture" Reasoning.

üßπ **Janitor-Service** - LLM-gest√ºtzte automatische Bereinigung mit hierarchischer Summarization, Observation-Pruning und **automatischer Session-Komprimierung**.

üóúÔ∏è **Kontext-Kompaktierung & Auto-Summarization (v2.2)** - Automatische und manuelle Memory-Konsolidierung mit progressiver Summarization und LLM-gest√ºtzten Executive Summaries.

üß† **Fact Lifecycle Management (v2.1)** - Natives ‚ÄûSoft-Deletion‚Äú via CozoDB Validity Retraction; ung√ºltige Fakten werden aus aktuellen Sichten ausgeblendet, bleiben aber in der Historie f√ºr Audits erhalten.

üë§ **User Preference Profiling** - Persistente User-Pr√§ferenzen mit automatischem 50% Search-Boost

üîç **Near-Duplicate Detection** - Automatische LSH-basierte Deduplizierung zur Vermeidung von Redundanz

üß† **Inference Engine** - Implizite Wissensentdeckung mit mehreren Strategien

üè† **100% Lokal** - Embeddings via ONNX/Transformers; keine externen Services erforderlich

üì¶ **Export/Import (seit v1.8)** - Export nach JSON, Markdown oder Obsidian-ready ZIP; Import von Mem0, MemGPT, Markdown oder nativem Format

üìÑ **PDF-Unterst√ºtzung (seit v1.9)** - Direkte PDF-Ingestion mit Textextraktion via pdfjs-dist; unterst√ºtzt Dateipfad und Content-Parameter

üïê **Duales Zeitstempel-Format (seit v1.9)** - Alle Zeitstempel werden sowohl als Unix-Mikrosekunden als auch im ISO 8601 Format zur√ºckgegeben

### Detaillierte Features

Dieses Repository enth√§lt:
- einen MCP-Server (stdio) f√ºr Claude/andere MCP-Clients,
- einen optionalen HTTP-API-Bridge-Server f√ºr UI/Tools,

Wesentliche Eigenschaften:
- **Hybride Suche (v0.7 Optimized)**: Kombination aus semantischer Suche (HNSW), **Full-Text Search (FTS)** und Graph-Signalen, zusammengef√ºhrt via Reciprocal Rank Fusion (RRF).
- **Full-Text Search (FTS)**: Native CozoDB v0.7 FTS-Indizes mit **englischem** Stemming, Stopword-Filterung und robustem Query-Sanitizing (Bereinigung von `+ - * / \ ( ) ? .`) f√ºr maximale Stabilit√§t.
- **Near-Duplicate Detection (LSH)**: Erkennt automatisch sehr √§hnliche Beobachtungen via MinHash-LSH (CozoDB v0.7), um Redundanz zu vermeiden.
- **Recency Bias**: √§ltere Inhalte werden in der Fusion ged√§mpft (au√üer bei expliziter Keyword-Suche), damit ‚Äûaktuell relevant‚Äú h√§ufiger oben landet.
- **Graph-RAG & Graph-Walking (v1.7 Optimized)**: Erweitertes Retrieval-Verfahren, das semantische Vektor-Seeds mit rekursiven Graph-Traversals kombiniert. Nutzt nun einen optimierten **Graph-Walking** Algorithmus via Datalog, der HNSW-Index-Lookups f√ºr pr√§zise Distanzberechnungen w√§hrend der Traversierung verwendet.
- **Multi-Vector Support (v1.7)**: Jede Entit√§t verf√ºgt nun √ºber zwei spezialisierte Vektoren:
  1. **Content-Embedding**: Repr√§sentiert den inhaltlichen Kontext (Beobachtungen).
  2. **Name-Embedding**: Optimiert f√ºr die Identifikation via Namen/Label.
  Dies verbessert die Genauigkeit beim Einstieg in Graph-Walks signifikant.
- **Semantic & Persistent Caching (v0.8.5)**: Zweistufiges Caching-System:
  1. **L1 Memory Cache**: Ultraschneller In-Memory LRU-Cache (< 0.1ms).
  2. **L2 Persistent Cache**: Speicherung in CozoDB f√ºr Neustart-Resistenz.
  3. **Semantic Matching**: Erkennt semantisch √§hnliche Queries via Vektor-Distanz.
  4. **Janitor TTL**: Automatische Bereinigung veralteter Cache-Eintr√§ge durch den Janitor-Service.
- **Time-Travel**: √Ñnderungen werden √ºber CozoDB `Validity` versioniert; historische Abfragen sind m√∂glich.
- **JSON Merge Operator (++)**: Nutzt den v0.7 Merge-Operator f√ºr effiziente, atomare Metadaten-Updates.
- **Multi-Statement Transactions (v1.2)**: Unterst√ºtzt atomare Transaktionen √ºber mehrere Operationen hinweg mittels CozoDB-Block-Syntax `{ ... }`. Dies garantiert, dass zusammenh√§ngende √Ñnderungen (z.B. Entity erstellen + Observation hinzuf√ºgen + Beziehung kn√ºpfen) entweder vollst√§ndig oder gar nicht ausgef√ºhrt werden.
- **Graph-Metriken & Ranking Boost (v1.3 / v1.6)**: Integriert fortgeschrittene Graph-Algorithmen:
  - **PageRank**: Berechnet die "Wichtigkeit" von Wissensknoten f√ºr das Ranking.
  - **Betweenness Centrality**: Identifiziert zentrale Br√ºckenelemente im Wissensnetzwerk.
  - **HITS (Hubs & Authorities)**: Unterscheidet zwischen Informationsquellen (Authorities) und Wegweisern (Hubs).
  - **Connected Components**: Erkennt isolierte Wissensinseln und Teilgraphen.
  - Diese Metriken werden automatisch in der Hybrid-Suche (`advancedSearch`) und im `graphRag` genutzt.
- **Native CozoDB Operatoren (v1.5)**: Verwendet nun explizite `:insert`, `:update` und `:delete` Operatoren anstelle von generischen `:put` (upsert) Aufrufen. Dies erh√∂ht die Datensicherheit durch strikte Validierung der Datenbankzust√§nde (z. B. Fehler beim Versuch, eine existierende Entit√§t erneut zu "inserten").
- **Advanced Time-Travel Analysis (v1.5)**: Erweiterung der Beziehungs-Historie um Zeitbereichs-Filter (`since`/`until`) und automatische Diff-Zusammenfassungen, um Ver√§nderungen (Hinzuf√ºgungen/Entfernungen) √ºber spezifische Zeitr√§ume hinweg zu analysieren.
- **Graph-Features (v1.6)**: Native Integration von Shortest Path (Dijkstra) mit Pfad-Rekonstruktion, Community Detection (LabelPropagation) und fortgeschrittenen Zentralit√§tsma√üen.
- **Graph-Evolution**: Trackt die zeitliche Entwicklung von Beziehungen (z. B. Rollenwechsel von ‚ÄûManager‚Äú zu ‚ÄûBerater‚Äú) via CozoDB `Validity` Queries.
- **Bridge Discovery**: Identifiziert ‚ÄûBr√ºcken-Entit√§ten‚Äú, die verschiedene Communities verbinden ‚Äì ideal f√ºr kreatives Brainstorming.
- **Inference**: implizite Vorschl√§ge und Kontext-Erweiterung (z. B. transitive Expertise-Regel).
- **Konflikterkennung (Application-Level & Triggers)**: Erkennt automatisch Widerspr√ºche in den Metadaten (z. B. ‚Äûaktiv‚Äú vs. ‚Äûeingestellt‚Äú / `archived: true`). Nutzt eine robuste Logik in der App-Schicht, um Datenintegrit√§t vor dem Schreiben sicherzustellen.
- **Datenintegrit√§t (Trigger-Konzept)**: Verhindert ung√ºltige Zust√§nde wie Selbst-Referenzen in Beziehungen (Self-Loops) direkt bei der Erstellung.
- **Hierarchische Summarization**: Der Janitor verdichtet alte Fragmente zu ‚ÄûExecutive Summary‚Äú-Knoten, um das ‚ÄûBig Picture‚Äú langfristig zu erhalten.
- **User Preference Profiling**: Eine spezialisierte `global_user_profile` Entit√§t speichert persistente Pr√§ferenzen (Vorlieben, Arbeitsstil), die bei jeder Suche einen **50% Score-Boost** erhalten.
- **Fact Lifecycle Management (v2.1)**: Nutzt den nativen **Validity Retraction** Mechanismus von CozoDB. Anstatt Daten destruktiv zu l√∂schen, werden Fakten durch einen `[timestamp, false]` Eintrag als ung√ºltig markiert.
  1. **Revisionssicherheit**: Man kann jederzeit ‚Äûzur√ºck in der Zeit‚Äú reisen, um zu sehen, was das System zu einem bestimmten Zeitpunkt wusste.
  2. **Konsistenz**: Alle Standard-Abfragen (Search, Graph-RAG, Inference) nutzen den `@ "NOW"` Filter, um zur√ºckgezogene Fakten automatisch auszuschlie√üen.
  3. **Atomare Retraction**: Ung√ºltigkeitserkl√§rungen k√∂nnen Teil einer Transaktion sein, was saubere Update-Pattern (altes ung√ºltig machen + neues einf√ºgen) erm√∂glicht.
- **Alles lokal**: Embeddings via Transformers/ONNX; kein externer Embedding-Dienst n√∂tig.

## Positionierung & Vergleich

Die meisten "Memory"-MCP-Server lassen sich in zwei Kategorien einteilen:
1.  **Simple Knowledge-Graphs**: CRUD-Operationen auf Tripeln, oft nur Textsuche.
2.  **Reine Vector-Stores**: Semantische Suche (RAG), aber wenig Verst√§ndnis f√ºr komplexe Beziehungen.

Dieser Server f√ºllt die L√ºcke dazwischen ("Sweet Spot"): Eine **lokale, datenbankgest√ºtzte Memory-Engine**, die Vektor-, Graph- und Keyword-Signale kombiniert.

### Vergleich mit anderen L√∂sungen

| Feature | **CozoDB Memory (Dieses Projekt)** | **Official Reference (`@modelcontextprotocol/server-memory`)** | **mcp-memory-service (Community)** | **Datenbank-Adapter (Qdrant/Neo4j)** |
| :--- | :--- | :--- | :--- | :--- |
| **Backend** | **CozoDB** (Graph + Vektor + Relational) | JSON-Datei (`memory.jsonl`) | SQLite / Cloudflare | Spezialisierte DB (nur Vektor o. Graph) |
| **Such-Logik** | **Agentisch (Auto-Route)**: Hybrid + Graph + Summaries | Nur Keyword / Exakter Graph-Match | Vektor + Keyword | Meist nur eine Dimension |
| **Inference** | **Ja**: Eingebaute Engine f√ºr implizites Wissen | Nein | Nein ("Dreaming" ist Konsolidierung) | Nein (nur Retrieval) |
| **Community** | **Ja**: Hierarchische Community Summaries | Nein | Nein | Nur Clustering (keine Summary) |
| **Time-Travel** | **Ja**: Abfragen zu jedem Zeitpunkt (`Validity`) | Nein (nur aktueller Stand) | Historie vorhanden, kein natives DB-Feature | Nein |
| **Wartung** | **Janitor**: LLM-gest√ºtzte Bereinigung | Manuell | Automatische Konsolidierung | Meist manuell |
| **Deployment** | **Lokal** (Node.js + Embedded DB) | Lokal (Docker/NPX) | Lokal oder Cloud | Ben√∂tigt oft externen DB-Server |

Der Kernvorteil ist **Intelligenz und Nachvollziehbarkeit**: Durch die Kombination eines **Agentic Retrieval Layers** mit **Hierarchical GraphRAG** kann das System sowohl spezifische Faktenfragen als auch breite thematische Abfragen mit viel h√∂herer Pr√§zision beantworten als reine Vektor-Stores.

## Performance & Benchmarks

Benchmarks auf einem Standard-Entwickler-Laptop (Windows, Node.js 20+, nur CPU):

| Metrik | Wert | Anmerkung |
| :--- | :--- | :--- |
| **Graph-Walking (Rekursiv)** | **~130 ms** | Vektor-Seed + Rekursive Datalog-Traversierung |
| **Graph-RAG (Breadth-First)** | **~335 ms** | Vektor-Seeds + 2-Hop Expansion |
| **Hybrid Search (Cache Hit)** | **< 0.1 ms** | **v0.8+ Semantic Cache** |
| **Hybrid Search (Kalt)** | **~35 ms** | FTS + HNSW + RRF-Fusion |
| **Vektor-Suche (Raw)** | **~51 ms** | Reine semantische Suche als Referenz |
| **FTS-Suche (Raw)** | **~12 ms** | Native Full-Text Search Performance |
| **Ingestion** | **~102 ms** | Pro Op (Schreiben + Embedding + FTS/LSH Indexing) |
| **RAM-Verbrauch** | **~1.7 GB** | Prim√§r durch lokales `Xenova/bge-m3` modell |

### Benchmarks ausf√ºhren

Du kannst die Performance auf deinem System mit dem integrierten Benchmark-Tool testen:

```bash
npm run benchmark
```

Dieses Tool (`src/benchmark.ts`) f√ºhrt folgende Tests durch:
1.  **Initialisierung**: Kaltstart-Dauer des Servers inkl. Modell-Loading.
2.  **Ingestion**: Massen-Import von Test-Entit√§ten und Beobachtungen (Durchsatz).
3.  **Search Performance**: Latenz-Messung f√ºr Hybrid Search vs. Raw Vector Search.
4.  **RRF Overhead**: Ermittlung der zus√§tzlichen Rechenzeit f√ºr die Fusion-Logik.
 
### Evaluations-Suite ausf√ºhren (RAG-Qualit√§t)
Um die Qualit√§t und den Recall der verschiedenen Retrieval-Strategien (Suche vs. Graph-RAG vs. Graph-Walking) zu bewerten, nutzen Sie die Evaluations-Suite:
 
```bash
npm run eval
 ```
 
Dieses Tool vergleicht die Strategien anhand eines synthetischen Datensatzes und misst **Recall@K**, **MRR** und **Latenz**.
 
| Methode | Recall@10 | Avg Latenz | Bestens geeignet f√ºr |
| :--- | :--- | :--- | :--- |
| **Graph-RAG** | **1.00** | **~32 ms** | Tiefes relationales Reasoning |
| **Graph-RAG (Reranked)** | **1.00** | **~36 ms** | Maximale Pr√§zision f√ºr relationale Daten |
| **Graph-Walking** | 1.00 | ~50 ms | Assoziative Pfad-Exploration |
| **Hybrid-Suche** | 1.00 | ~89 ms | Breite Fakten-Abfrage |
| **Reranked Search** | 1.00 | ~20 ms* | Ultra-pr√§zise Faktensuche (Warm Cache) |

## Architektur (high level)

```mermaid
graph TB
    Client[MCP Client<br/>Claude Desktop, etc.]
    Server[MCP Server<br/>FastMCP + Zod Schemas]
    Services[Memory Services]
    Embeddings[Embeddings<br/>ONNX Runtime]
    Search[Hybrid Search<br/>RRF Fusion]
    Cache[Semantic Cache<br/>L1 + L2]
    Inference[Inference Engine<br/>Multi-Strategy]
    DB[(CozoDB SQLite<br/>Relations + Validity<br/>HNSW Indizes<br/>Datalog/Graph)]
    
    Client -->|stdio| Server
    Server --> Services
    Services --> Embeddings
    Services --> Search
    Services --> Cache
    Services --> Inference
    Services --> DB
    
    style Client fill:#e1f5ff,color:#000
    style Server fill:#fff4e1,color:#000
    style Services fill:#f0e1ff,color:#000
    style DB fill:#e1ffe1,color:#000
```

### Graph-Walking Visualisierung

```mermaid
graph LR
    Start([Query: Woran arbeitet Alice?])
    V1[Vektor-Suche<br/>Finde: Alice]
    E1[Alice<br/>Person]
    E2[Projekt X<br/>Projekt]
    E3[Feature Flags<br/>Technologie]
    E4[Bob<br/>Person]
    
    Start --> V1
    V1 -.semantische √Ñhnlichkeit.-> E1
    E1 -->|works_on| E2
    E2 -->|uses_tech| E3
    E1 -->|colleague_of| E4
    E4 -.semantisch: auch relevant.-> E2
    
    style Start fill:#e1f5ff,color:#000
    style V1 fill:#fff4e1,color:#000
    style E1 fill:#ffe1e1,color:#000
    style E2 fill:#e1ffe1,color:#000
    style E3 fill:#f0e1ff,color:#000
    style E4 fill:#ffe1e1,color:#000
```

## Installation

### Voraussetzungen
- Node.js 20+ (empfohlen)
- Die CozoDB Native Dependency wird via `cozo-node` installiert.

### Setup

```bash
npm install
npm run build
```

### Windows Quickstart

```bash
npm install
npm run build
npm run start
```

Hinweise:
- Beim ersten Start l√§dt `@xenova/transformers` das Embedding-Modell (kann dauern).
- Die Embeddings werden auf der CPU verarbeitet.

## Start / Integration

### MCP Server (stdio)

Der MCP Server l√§uft √ºber stdio (f√ºr Claude Desktop & Co.). Start:

```bash
npm run start
```

Standard-Datenbankpfad: `memory_db.cozo.db` im Projektroot (wird automatisch angelegt).

### Claude Desktop Integration

```json
{
  "mcpServers": {
    "cozo-memory": {
      "command": "node",
      "args": ["C:/Pfad/zu/cozo-memory/dist/index.js"]
    }
  }
}
```

### CLI-Tool

CozoDB Memory enth√§lt ein vollwertiges CLI f√ºr alle Operationen:

```bash
# System-Operationen
cozo-memory system health
cozo-memory system metrics
cozo-memory system reflect

# Entit√§ts-Operationen
cozo-memory entity create -n "MyEntity" -t "person" -m '{"age": 30}'
cozo-memory entity get -i <entity-id>
cozo-memory entity delete -i <entity-id>

# Beobachtungen (Observations)
cozo-memory observation add -i <entity-id> -t "Notiz"

# Beziehungen (Relations)
cozo-memory relation create --from <id1> --to <id2> --type "knows" -s 0.8

# Suche
cozo-memory search query -q "Suchbegriff" -l 10
cozo-memory search context -q "Kontext-Abfrage"
cozo-memory search agentic -q "Agentic-Suche"

# Graph-Operationen
cozo-memory graph explore -s <entity-id> -h 3
cozo-memory graph pagerank
cozo-memory graph communities
cozo-memory graph summarize

# Session- & Task-Management
cozo-memory session start -n "Meine Session"
cozo-memory session stop -i <session-id>
cozo-memory task start -n "Mein Task" -s <session-id>
cozo-memory task stop -i <task-id>

# Export/Import
cozo-memory export json -o backup.json --include-metadata --include-relationships --include-observations
cozo-memory export markdown -o notes.md
cozo-memory export obsidian -o vault.zip
cozo-memory import file -i data.json -f cozo

# Alle Befehle unterst√ºtzen -f json oder -f pretty f√ºr die Formatierung
```

## Konfiguration & Backends

Das System unterst√ºtzt verschiedene Speicher-Backends. Standardm√§√üig wird **SQLite** verwendet, da es keine zus√§tzliche Installation erfordert und f√ºr die meisten Anwendungsf√§lle die beste Balance aus Performance und Einfachheit bietet.

### Backend wechseln (z. B. zu RocksDB)

RocksDB bietet Vorteile bei sehr gro√üen Datens√§tzen (Millionen von Eintr√§gen) und schreibintensiven Workloads durch bessere Parallelit√§t und Datenkompression.

Um das Backend zu wechseln, setzen Sie die Umgebungsvariable `DB_ENGINE` vor dem Start:

**PowerShell:**
```powershell
$env:DB_ENGINE="rocksdb"; npm run dev
```

**Bash:**
```bash
DB_ENGINE=rocksdb npm run dev
```

| Backend | Status | Empfehlung |
| :--- | :--- | :--- |
| **SQLite** | Aktiv (Standard) | Standard f√ºr Desktop/lokale Nutzung. |
| **RocksDB** | Vorbereitet & Getestet | F√ºr High-Performance oder sehr gro√üe Datenmengen. |
| **MDBX** | Nicht unterst√ºtzt | Erfordert manuellen Build von `cozo-node` aus dem Quellcode. |

---

## Datenmodell

CozoDB-Relations (vereinfacht) ‚Äì alle Schreiboperationen erzeugen neue `Validity`-Eintr√§ge (Time-Travel):
- `entity`: `id`, `created_at: Validity` ‚áí `name`, `type`, `embedding(1024)`, `name_embedding(1024)`, `metadata(Json)`
- `observation`: `id`, `created_at: Validity` ‚áí `entity_id`, `text`, `embedding(1024)`, `metadata(Json)`
- `relationship`: `from_id`, `to_id`, `relation_type`, `created_at: Validity` ‚áí `strength(0..1)`, `metadata(Json)`
- `entity_community`: `entity_id` ‚áí `community_id` (Key-Value Mapping aus LabelPropagation)
- `memory_snapshot`: `snapshot_id` ‚áí Counts + `metadata` + `created_at(Int)`

## MCP Tools

Die Oberfl√§che ist auf **4 konsolidierte Tools** reduziert. Die konkrete Operation wird immer √ºber `action` gew√§hlt.

| Tool | Zweck | Wichtige Aktionen |
|------|-------|-------------------|
| `mutate_memory` | Schreiboperationen | create_entity, update_entity, delete_entity, add_observation, create_relation, run_transaction, add_inference_rule, ingest_file, invalidate_observation, invalidate_relation |
| `query_memory` | Leseoperationen | search, advancedSearch, context, entity_details, history, graph_rag, graph_walking, agentic_search |
| `analyze_graph` | Graph-Analyse | explore, communities, pagerank, betweenness, hits, shortest_path, bridge_discovery, semantic_walk, infer_relations |
| `manage_system` | Wartung | health, metrics, export_memory, import_memory, snapshot_create, snapshot_list, snapshot_diff, cleanup, defrag, reflect, summarize_communities, clear_memory, compact |

### mutate_memory (Schreiben)

Aktionen:
- `create_entity`: `{ name, type, metadata? }`
- `update_entity`: `{ id, name?, type?, metadata? }`
- `delete_entity`: `{ entity_id }`
- `add_observation`: `{ entity_id?, entity_name?, entity_type?, text, metadata? }`
- `create_relation`: `{ from_id, to_id, relation_type, strength?, metadata? }`
- `run_transaction`: `{ operations: Array<{ action, params }> }` **(Neu v1.2)**: F√ºhrt mehrere Operationen atomar aus.
- `add_inference_rule`: `{ name, datalog }`
- `ingest_file`: `{ format, file_path?, content?, entity_id?, entity_name?, entity_type?, chunking?, metadata?, observation_metadata?, deduplicate?, max_observations? }`
- `invalidate_observation`: `{ observation_id }` **(Neu v2.1)**: Zieht eine Observation mittels Validity `[now, false]` zur√ºck.
- `invalidate_relation`: `{ from_id, to_id, relation_type }` **(Neu v2.1)**: Zieht eine Beziehung mittels Validity `[now, false]` zur√ºck.
  - `format` Optionen: `"markdown"`, `"json"`, `"pdf"` **(Neu v1.9)**
  - `file_path`: Optionaler Pfad zur Datei auf der Festplatte (Alternative zum `content` Parameter)
  - `content`: Dateiinhalt als String (erforderlich wenn `file_path` nicht angegeben)
  - `chunking` Optionen: `"none"`, `"paragraphs"` (zuk√ºnftig: `"semantic"`)

Wichtige Details:
- `run_transaction` unterst√ºtzt `create_entity`, `add_observation` und `create_relation`. Parameter werden automatisch suffigiert, um Kollisionen zu vermeiden.
- `create_relation` lehnt Selbst-Referenzen (`from_id === to_id`) ab.
- `strength` ist optional und defaultet auf `1.0`.
- `add_observation` liefert zus√§tzlich `inferred_suggestions` (Vorschl√§ge aus der Inference Engine).
- `add_observation` macht Deduplication (exakt + semantisch via LSH). Bei Duplikaten kommt `status: "duplicate_detected"` mit `existing_observation_id` und einer gesch√§tzten `similarity`.
- `update_entity` nutzt den JSON-Merge Operator `++` (v0.7), um bestehende Metadaten mit den neuen Werten zu vereinen, anstatt sie zu √ºberschreiben.
- `add_inference_rule` validiert den Datalog-Code beim Speichern. Ung√ºltige Syntax oder fehlende Pflichtspalten f√ºhren zu einem Fehler.

Beispiele:

```json
{ "action": "create_entity", "name": "Alice", "type": "Person", "metadata": { "role": "Dev" } }
```

```json
{ "action": "add_observation", "entity_id": "ENTITY_ID", "text": "Alice arbeitet am Feature-Flag-System." }
```

Beispiel (Duplikat):

```json
{ "action": "add_observation", "entity_id": "ENTITY_ID", "text": "Alice arbeitet am Feature-Flag-System." }
```

```json
{ "status": "duplicate_detected", "existing_observation_id": "OBS_ID", "similarity": 1 }
```

```json
{ "action": "create_relation", "from_id": "ALICE_ID", "to_id": "PROJ_ID", "relation_type": "works_on", "strength": 1.0 }
```

Custom Datalog Regeln (Inference):

- Inferenzregeln werden als `action: "add_inference_rule"` gespeichert.
- Die Datalog-Query muss ein Resultset mit **genau diesen 5 Spalten** liefern: `from_id, to_id, relation_type, confidence, reason`.
- `$id` ist der Platzhalter f√ºr die Entity-ID, f√ºr die die Inferenz gestartet wird.
- Tabellen im Schema:
    - `*entity{id, name, type, metadata, @ "NOW"}`
    - `*relationship{from_id, to_id, relation_type, strength, metadata, @ "NOW"}`
    - `*observation{id, entity_id, text, metadata, @ "NOW"}`

Beispiel (Transitiver Manager ‚áí Ober-Manager):

```json
{
  "action": "add_inference_rule",
  "name": "ober_manager",
  "datalog": "?[from_id, to_id, relation_type, confidence, reason] :=\n  *relationship{from_id: $id, to_id: mid, relation_type: \"manager_of\", @ \"NOW\"},\n  *relationship{from_id: mid, to_id: target, relation_type: \"manager_of\", @ \"NOW\"},\n  from_id = $id,\n  to_id = target,\n  relation_type = \"ober_manager_von\",\n  confidence = 0.6,\n  reason = \"Transitiver Manager-Pfad gefunden\""
}
```

Beispiel (Reziproke Beziehung vorschlagen):

```json
{
  "action": "add_inference_rule",
  "name": "colleague_symmetry",
  "datalog": "?[from_id, to_id, relation_type, confidence, reason] :=\n  *relationship{from_id: other, to_id: $id, relation_type: \"colleague_of\", @ \"NOW\"},\n  from_id = $id,\n  to_id = other,\n  relation_type = \"colleague_of\",\n  confidence = 0.9,\n  reason = \"Symmetrische Kollegenbeziehung\""
}
```

Regeln testen (liefert Vorschl√§ge, persistiert sie aber nicht automatisch):

```json
{ "action": "infer_relations", "entity_id": "ENTITY_ID" }
```

Bulk-Ingestion (Markdown/JSON/PDF):

```json
{
  "action": "ingest_file",
  "entity_name": "Projektdokumentation",
  "format": "markdown",
  "chunking": "paragraphs",
  "content": "# Titel\n\nAbschnitt 1...\n\nAbschnitt 2...",
  "deduplicate": true,
  "max_observations": 50
}
```

PDF-Ingestion √ºber Dateipfad:

```json
{
  "action": "ingest_file",
  "entity_name": "Forschungsarbeit",
  "format": "pdf",
  "file_path": "/pfad/zur/dokument.pdf",
  "chunking": "paragraphs",
  "deduplicate": true
}
```

### query_memory (Lesen)

Aktionen:
- `search`: `{ query, limit?, entity_types?, include_entities?, include_observations?, rerank? }`
- `advancedSearch`: `{ query, limit?, filters?, graphConstraints?, vectorOptions?, rerank? }`
- `context`: `{ query, context_window?, time_range_hours? }`
- `entity_details`: `{ entity_id, as_of? }`
- `history`: `{ entity_id }`
- `graph_rag`: `{ query, max_depth?, limit?, filters?, rerank? }` Graph-basiertes Reasoning. Findet erst Vektor-Seeds (mit Inline-Filtering) und expandiert dann transitive Beziehungen. Nutzt rekursives Datalog f√ºr effiziente BFS-Expansion.
- `graph_walking`: `{ query, start_entity_id?, max_depth?, limit? }` (v1.7) Rekursive semantische Graph-Suche. Startet bei Vektor-Seeds oder einer spezifischen Entit√§t und folgt Beziehungen zu anderen semantisch relevanten Entit√§ten. Ideal f√ºr tiefere Pfad-Exploration.
- `agentic_search`: `{ query, limit?, rerank? }` **(Neu v2.0)**: **Auto-Routing Suche**. Nutzt ein lokales LLM (Ollama), um den Query-Intent zu analysieren und routet die Anfrage automatisch zur am besten geeigneten Strategie (`vector_search`, `graph_walk`, oder `community_summary`).
- `dynamic_fusion`: `{ query, config?, limit? }` **(Neu v2.3)**: **Dynamic Fusion Framework**. Kombiniert 4 Retrieval-Pfade (Dense Vector, Sparse Vector, FTS, Graph) mit konfigurierbaren Gewichten und Fusion-Strategien. Inspiriert von Allan-Poe (arXiv:2511.00855).
- `get_relation_evolution`: `{ from_id, to_id?, since?, until? }` (in `analyze_graph`) Zeigt die zeitliche Entwicklung von Beziehungen inklusive Zeitbereichs-Filter und Diff-Zusammenfassung.

Wichtige Details:
- `advancedSearch` erlaubt pr√§zise Filterung:
    - `filters.entityTypes`: Liste von Entit√§tstypen.
    - `filters.metadata`: Key-Value Map f√ºr exakte Metadaten-Treffer.
    - `graphConstraints.requiredRelations`: Nur Entit√§ten, die bestimmte Beziehungen haben.
    - `graphConstraints.targetEntityIds`: Nur Entit√§ten, die mit diesen Ziel-IDs verbunden sind.
- `context` liefert ein JSON-Objekt mit Entities, Observations, Graph-Verbindungen und Inferenz-Vorschl√§gen.
- `search` nutzt RRF (Reciprocal Rank Fusion), um Vektor- und Keyword-Signale zu mischen.
- `graph_rag` kombiniert Vektor-Suche mit graph-basierten Traversals (Standard-Tiefe 2) f√ºr "strukturiertes Reasoning". Die Expansion erfolgt bidirektional √ºber alle Beziehungstypen.
- **User Profiling**: Ergebnisse, die mit der `global_user_profile` Entit√§t verkn√ºpft sind, werden automatisch bevorzugt (Boost).
- `time_range_hours` filtert die Ergebnis-Kandidaten im Zeitfenster (in Stunden, kann float sein).
- `as_of` akzeptiert ISO-Strings oder `"NOW"`; invalides Format f√ºhrt zu einem Fehler.
- Bei erkannten Status-Widerspr√ºchen wird optional `conflict_flag` an Entities/Observations angeh√§ngt; `context` liefert zus√§tzlich `conflict_flags` als Zusammenfassung.

Beispiele:

```json
{ "action": "search", "query": "Feature Flag", "limit": 10 }
```

```json
{ 
  "action": "advancedSearch", 
  "query": "Manager", 
  "filters": { "metadata": { "role": "Lead" } },
  "graphConstraints": { "requiredRelations": ["works_with"] }
}
```

```json
{ "action": "graph_rag", "query": "Woran arbeitet Alice?", "max_depth": 2 }
```

```json
{ "action": "context", "query": "Woran arbeitet Alice gerade?", "context_window": 20 }
```

```json
{ "action": "context", "query": "Woran arbeitet Alice gerade?", "context_window": 20, "time_range_hours": 24 }
```

```json
{ "action": "entity_details", "entity_id": "ENTITY_ID", "as_of": "2026-02-01T12:00:00Z" }
```

#### Dynamic Fusion Framework (v2.3)

Das Dynamic Fusion Framework kombiniert 4 Retrieval-Pfade mit konfigurierbaren Gewichten und Fusion-Strategien:

**Retrieval-Pfade:**
1. **Dense Vector Search (HNSW)**: Semantische √Ñhnlichkeit via Embeddings
2. **Sparse Vector Search**: Keyword-basiertes Matching mit TF-IDF Scoring
3. **Full-Text Search (FTS)**: BM25 Scoring auf Entity-Namen
4. **Graph Traversal**: Multi-Hop Beziehungsexpansion von Vektor-Seeds

**Fusion-Strategien:**
- `rrf` (Reciprocal Rank Fusion): Kombiniert Rankings mit positionsbasiertem Scoring
- `weighted_sum`: Direkte gewichtete Kombination von Scores
- `max`: Nimmt maximalen Score √ºber alle Pfade
- `adaptive`: Query-abh√§ngige Gewichtung (zuk√ºnftige Erweiterung)

**Konfigurations-Beispiel:**

```json
{
  "action": "dynamic_fusion",
  "query": "Datenbank mit Graph-F√§higkeiten",
  "limit": 10,
  "config": {
    "vector": {
      "enabled": true,
      "weight": 0.4,
      "topK": 20,
      "efSearch": 100
    },
    "sparse": {
      "enabled": true,
      "weight": 0.3,
      "topK": 20,
      "minScore": 0.1
    },
    "fts": {
      "enabled": true,
      "weight": 0.2,
      "topK": 20,
      "fuzzy": true
    },
    "graph": {
      "enabled": true,
      "weight": 0.1,
      "maxDepth": 2,
      "maxResults": 20,
      "relationTypes": ["related_to", "uses"]
    },
    "fusion": {
      "strategy": "rrf",
      "rrfK": 60,
      "minScore": 0.0,
      "deduplication": true
    }
  }
}
```

**Antwort enth√§lt:**
- `results`: Fusionierte und gerankte Ergebnisse mit Pfad-Beitrags-Details
- `stats`: Performance-Metriken inklusive:
  - `totalResults`: Anzahl der Ergebnisse nach Fusion
  - `pathContributions`: Anzahl der Ergebnisse pro Pfad
  - `fusionTime`: Gesamte Ausf√ºhrungszeit
  - `pathTimes`: Individuelle Ausf√ºhrungszeiten pro Pfad

**Anwendungsf√§lle:**
- **Breite Exploration**: Alle Pfade aktiviert mit ausgewogenen Gewichten
- **Pr√§zisions-Suche**: Hohes Vektor-Gewicht, niedriges Graph-Gewicht
- **Beziehungs-Entdeckung**: Hohes Graph-Gewicht mit spezifischen Beziehungstypen
- **Keyword-Matching**: Hohe Sparse/FTS-Gewichte f√ºr exakte Term-√úbereinstimmung

```json

#### Konflikterkennung (Status)

Wenn es f√ºr eine Entity widerspr√ºchliche Aussagen zum Status gibt, wird ein Konflikt markiert. Dabei ber√ºcksichtigt das System **temporale Konsistenz**:

- **Status-Widerspruch**: Eine Entity hat im **gleichen Kalenderjahr** sowohl den Status ‚Äûaktiv‚Äú als auch ‚Äûinaktiv‚Äú.
- **Status-√Ñnderung (Kein Konflikt)**: Wenn die Aussagen aus unterschiedlichen Jahren stammen (z.B. 2024 ‚Äûeingestellt‚Äú, 2025 ‚Äûaktiv‚Äú), wird dies als legitime √Ñnderung interpretiert und **nicht** als Konflikt markiert.

Die Erkennung nutzt Regex-Matching auf Keywords wie:
- **Aktiv**: aktiv, l√§uft, ongoing, active, running, in betrieb, fortgesetzt, weiter gef√ºhrt, nicht eingestellt.
- **Inaktiv**: eingestellt, abgebrochen, gestoppt, stillgelegt, geschlossen, shutdown, deprecated, archiviert, beendet, aufgegeben.

**Integration in API-Antworten:**
- `entities[i].conflict_flag` bzw. `observations[i].conflict_flag`: Flag direkt am Treffer.
- `conflict_flags`: Liste aller erkannten Konflikte im `context`- oder `search`-Result.

Beispiel f√ºr einen erkannten Konflikt (gleiches Jahr):

```json
{
  "entities": [
    {
      "id": "‚Ä¶",
      "name": "Projekt X",
      "type": "Project",
      "conflict_flag": {
        "entity_id": "‚Ä¶",
        "entity_name": "Projekt X",
        "entity_type": "Project",
        "kind": "status",
        "summary": "Konflikt: Es gibt widerspr√ºchliche Infos zum Status von Projekt X im gleichen Zeitraum (2026).",
        "evidence": {
          "active": { "created_at": 1767225600000000, "year": 2026, "text": "Projekt X ist aktiv." },
          "inactive": { "created_at": 1769904000000000, "year": 2026, "text": "Projekt X ist gestoppt." }
        }
      }
    }
  ]
}
```

### analyze_graph (Analyse)

Aktionen:
- `explore`: `{ start_entity, end_entity?, max_hops?, relation_types? }`
    - mit `end_entity`: k√ºrzester Pfad (BFS)
    - ohne `end_entity`: Nachbarschaft bis max. 5 Hops (aggregiert nach minimaler Hop-Anzahl)
- `communities`: `{}` berechnet Communities neu und schreibt `entity_community`
- `pagerank`: `{}` Berechnet PageRank-Scores f√ºr alle Entit√§ten.
- `betweenness`: `{}` Berechnet Betweenness Centrality (Zentralit√§tsma√ü f√ºr Br√ºckenelemente).
- `hits`: `{}` Berechnet HITS-Scores (Hubs & Authorities).
- `connected_components`: `{}` Identifiziert isolierte Teilgraphen.
- `shortest_path`: `{ start_entity, end_entity }` Berechnet den k√ºrzesten Pfad via Dijkstra (inkl. Distanz und Pfad-Rekonstruktion).
- `bridge_discovery`: `{}` Sucht nach Entit√§ten, die als Br√ºcken zwischen isolierten Communities fungieren (hohe Betweenness-Relevanz)
- `semantic_walk`: `{ start_entity, max_depth?, min_similarity? }` (v1.7) Rekursive semantische Graph-Suche. Startet bei einer Entit√§t und folgt rekursiv Pfaden, die aus expliziten Beziehungen UND semantischer √Ñhnlichkeit (Vektor-Distanz) bestehen. Findet "assoziative Pfade" im Wissensgraphen.
- `hnsw_clusters`: `{}` Analysiert Cluster direkt auf dem HNSW-Graphen (Layer 0). Extrem schnell, da keine Vektorberechnungen n√∂tig sind.
- `infer_relations`: `{ entity_id }` liefert Vorschl√§ge aus mehreren Strategien
- `get_relation_evolution`: `{ from_id, to_id?, since?, until? }` zeigt die zeitliche Entwicklung von Beziehungen inklusive Zeitbereichs-Filter und Diff-Zusammenfassung.

Beispiele:

```json
{ "action": "shortest_path", "start_entity": "ID_A", "end_entity": "ID_B" }
```

```json
{ "action": "hits" }
```

```json
{ "action": "explore", "start_entity": "ENTITY_ID", "max_hops": 3 }
```

```json
{ "action": "get_relation_evolution", "from_id": "ALICE_ID", "to_id": "PROJECT_X_ID" }
```

```json
{ "action": "infer_relations", "entity_id": "ENTITY_ID" }
```

### manage_system (Wartung)

Aktionen:
- `health`: `{}` liefert DB-Counts + Embedding-Cache-Stats + Performance-Metriken.
- `metrics`: `{}` liefert detaillierte Operationsz√§hler, Fehlerstatistiken und Performance-Daten.
- `export_memory`: `{ format, includeMetadata?, includeRelationships?, includeObservations?, entityTypes?, since? }` exportiert Memory in verschiedene Formate.
- `import_memory`: `{ data, sourceFormat, mergeStrategy?, defaultEntityType? }` importiert Memory aus externen Quellen.
- `snapshot_create`: `{ metadata? }`
- `snapshot_list`: `{}`
- `snapshot_diff`: `{ snapshot_id_a, snapshot_id_b }`
- `cleanup`: `{ confirm, older_than_days?, max_observations?, min_entity_degree?, model? }`
- `compact`: `{ session_id?, entity_id?, model? }` **(Neu v2.2)**: Manuelle Kontext-Kompaktierung. Unterst√ºtzt drei Modi:
  - **Session-Kompaktierung**: `{ session_id, model? }` - Fasst Session-Observations in 2-3 Bullet Points zusammen und speichert im User-Profil
  - **Entity-Kompaktierung**: `{ entity_id, model? }` - Komprimiert Entity-Observations bei Schwellenwert-√úberschreitung, erstellt Executive Summary
  - **Globale Kompaktierung**: `{}` (keine Parameter) - Komprimiert alle Entities, die den Schwellenwert √ºberschreiten (Standard: 20 Observations)
- `summarize_communities`: `{ model?, min_community_size? }` **(Neu v2.0)**: Triggert die **Hierarchical GraphRAG** Pipeline. Berechnet Communities neu, generiert thematische Zusammenfassungen via LLM und speichert diese als `CommunitySummary` Entit√§ten.
- `reflect`: `{ entity_id?, mode?, model? }` Analysiert Memory auf Widerspr√ºche und neue Einsichten. Unterst√ºtzt die Modi `summary` (Standard) und `discovery` (autonome Beziehungs-Refinerung).
- `clear_memory`: `{ confirm }`

Janitor-Cleanup Details:
- `cleanup` unterst√ºtzt `dry_run`: bei `confirm: false` werden nur Kandidaten gelistet.
- Bei `confirm: true` wird der Janitor aktiv:
  - **Hierarchische Summarization**: Erkennt isolierte oder alte Beobachtungen, l√§sst sie von einer lokalen LLM (Ollama) zusammenfassen und erstellt einen neuen `ExecutiveSummary`-Knoten. Die alten Fragmente werden gel√∂scht, um Rauschen zu reduzieren, w√§hrend das Wissen erhalten bleibt.

Kontext-Kompaktierung Details **(Neu v2.2)**:
- **Automatische Kompaktierung**: Wird automatisch ausgel√∂st, wenn Observations den Schwellenwert √ºberschreiten (Standard: 20)
  - L√§uft im Hintergrund w√§hrend `addObservation`
  - Verwendet Lock-Mechanismus zur Vermeidung gleichzeitiger Kompaktierung
- **Manuelle Kompaktierung**: Verf√ºgbar via `compact` Action in `manage_system`
  - **Session-Modus**: Fasst Session-Observations zusammen und speichert im `global_user_profile`
  - **Entity-Modus**: Komprimiert spezifische Entity mit anpassbarem Schwellenwert
  - **Globaler Modus**: Komprimiert alle Entities, die den Schwellenwert √ºberschreiten
- **Progressive Summarization**: Neue Observations werden mit existierenden Executive Summaries gemergt statt einfach angeh√§ngt
- **LLM-Integration**: Verwendet Ollama (Standard-Modell: `demyagent-4b-i1:Q6_K`) f√ºr intelligente Zusammenfassung

**Vor Janitor:**
```
Entity: Projekt X
‚îú‚îÄ Observation 1: "Gestartet in Q1" (90 Tage alt, isoliert)
‚îú‚îÄ Observation 2: "Nutzt React" (85 Tage alt, isoliert)
‚îú‚îÄ Observation 3: "Team von 5" (80 Tage alt, isoliert)
‚îî‚îÄ Observation 4: "Deployed auf Staging" (75 Tage alt, isoliert)
```

**Nach Janitor:**
```
Entity: Projekt X
‚îî‚îÄ ExecutiveSummary: "Projekt X ist eine React-basierte Anwendung, die in Q1 
   mit einem Team von 5 Entwicklern gestartet wurde und aktuell auf der 
   Staging-Umgebung deployed ist."
```

### Self-Improving Memory Loop (`reflect`)
`reflect` analysiert Beobachtungen einer Entit√§t (oder der Top 5 aktivsten Entit√§ten), um Widerspr√ºche, Muster oder zeitliche Entwicklungen zu finden.
- **Modi**:
  - `summary` (Standard): Erzeugt eine textuelle "Reflexive Einsicht" Beobachtung.
  - `discovery`: Sucht autonom nach potenziellen Beziehungen mittels Inference Engine und validiert diese via LLM. 
    - Hochkonfidente Links (>0,8) werden automatisch erstellt.
    - Mittelkonfidente Links (>0,5) werden als Vorschlag (Suggestions) zur√ºckgegeben.
- Ergebnisse werden als neue Beobachtungen (bei `summary`) oder Beziehungen (bei `discovery`) mit dem Metadaten-Feld `{ "kind": "reflection" }` oder `{ "source": "reflection" }` persistiert.
- Der Text wird mit dem Pr√§fix `Reflexive Einsicht: ` gespeichert.

Defaults: `older_than_days=30`, `max_observations=20`, `min_entity_degree=2`, `model="demyagent-4b-i1:Q6_K"`.

Export/Import Details:
- `export_memory` unterst√ºtzt drei Formate:
  - **JSON** (`format: "json"`): Natives Cozo-Format, vollst√§ndig re-importierbar mit allen Metadaten und Zeitstempeln.
  - **Markdown** (`format: "markdown"`): Menschenlesbares Dokument mit Entities, Observations und Relationships.
  - **Obsidian** (`format: "obsidian"`): ZIP-Archiv mit Wiki-Links `[[Entity]]`, YAML-Frontmatter, bereit f√ºr Obsidian-Vault.
- `import_memory` unterst√ºtzt vier Quellformate:
  - **Cozo** (`sourceFormat: "cozo"`): Import aus nativem JSON-Export.
  - **Mem0** (`sourceFormat: "mem0"`): Import aus Mem0-Format (user_id wird zu Entity).
  - **MemGPT** (`sourceFormat: "memgpt"`): Import aus MemGPT Archival/Recall Memory.
  - **Markdown** (`sourceFormat: "markdown"`): Parse Markdown-Abschnitte als Entities mit Observations.
- Merge-Strategien: `skip` (Standard, Duplikate √ºberspringen), `overwrite` (Existierende ersetzen), `merge` (Metadaten kombinieren).
- Optionale Filter: `entityTypes` (Array), `since` (Unix-Timestamp in ms), `includeMetadata`, `includeRelationships`, `includeObservations`.

Beispiel Export:
```json
{
  "action": "export_memory",
  "format": "obsidian",
  "includeMetadata": true,
  "entityTypes": ["Person", "Project"]
}
```

Beispiel Import:
```json
{
  "action": "import_memory",
  "sourceFormat": "mem0",
  "data": "{\"user_id\": \"alice\", \"memories\": [...]}",
  "mergeStrategy": "skip"
}
```

Production Monitoring Details:
- `health` liefert umfassenden Systemstatus inklusive Entity/Observation/Relationship-Counts, Embedding-Cache-Statistiken und Performance-Metriken (letzte Operationszeit, durchschnittliche Operationszeit, Gesamtanzahl Operationen).
- `metrics` liefert detaillierte Betriebsmetriken:
  - **Operationsz√§hler**: Trackt create_entity, update_entity, delete_entity, add_observation, create_relation, search und graph_operations.
  - **Fehlerstatistiken**: Gesamtfehler und Aufschl√ºsselung nach Operationstyp.
  - **Performance-Metriken**: Dauer der letzten Operation, durchschnittliche Operationsdauer und Gesamtanzahl ausgef√ºhrter Operationen.
- Delete-Operationen enthalten nun detailliertes Logging mit Verifikationsschritten und liefern Statistiken √ºber gel√∂schte Daten (Observations, ausgehende/eingehende Relationen).

Beispiele:

```json
{ "action": "health" }
```

```json
{ "action": "reflect", "entity_id": "ENTITY_ID" }
```

```json
{ "action": "cleanup", "confirm": false, "older_than_days": 60, "max_observations": 25 }
```

**Memory Defragmentation (v2.3):**

```json
{ "action": "defrag", "confirm": false, "similarity_threshold": 0.95, "min_island_size": 3 }
```

Die `defrag` Action reorganisiert die Memory-Struktur durch:
1. **Duplikat-Erkennung**: Findet und merged near-duplicate Observations via Cosine-Similarity (Threshold 0.8-1.0, Standard 0.95)
2. **Insel-Verbindung**: Verbindet kleine Wissensinseln (‚â§3 Nodes) mit dem Hauptgraphen via semantische Br√ºcken
3. **Orphan-Entfernung**: L√∂scht verwaiste Entities ohne Observations oder Relations

Mit `confirm: false` wird ein Dry-Run durchgef√ºhrt, der Kandidaten anzeigt ohne √Ñnderungen vorzunehmen.

```json
{ "action": "snapshot_diff", "snapshot_id_a": "SNAP_A", "snapshot_id_b": "SNAP_B" }
```

```json
{ "action": "clear_memory", "confirm": true }
```

## Production Monitoring

Das System enth√§lt umfassende Monitoring-Funktionen f√ºr Produktionsumgebungen:

### Metriken-Tracking

Alle Operationen werden automatisch mit detaillierten Metriken erfasst:
- Operationsz√§hler nach Typ (create, update, delete, search, etc.)
- Fehler-Tracking mit Aufschl√ºsselung nach Operation
- Performance-Metriken (Latenz, Durchsatz)

### Health Endpoint

Die `health`-Aktion liefert Echtzeit-Systemstatus:
```json
{ "action": "health" }
```

Liefert:
- Datenbank-Counts (Entities, Observations, Relationships)
- Embedding-Cache-Statistiken (Hit-Rate, Gr√∂√üe)
- Performance-Metriken (letzte Operationszeit, Durchschnittszeit, Gesamtoperationen)

### Metrics Endpoint

Die `metrics`-Aktion liefert detaillierte Betriebsmetriken:
```json
{ "action": "metrics" }
```

Liefert:
- **operations**: Anzahl jedes Operationstyps
- **errors**: Gesamtfehler und Aufschl√ºsselung nach Operation
- **performance**: Dauer der letzten Operation, Durchschnittsdauer, Gesamtoperationen

### Erweiterte Delete-Operationen

Delete-Operationen enthalten umfassendes Logging und Verifikation:
- Detailliertes Schritt-f√ºr-Schritt-Logging mit `[Delete]`-Pr√§fix
- Z√§hlt zugeh√∂rige Daten vor dem L√∂schen
- Verifikation nach dem L√∂schen
- Liefert Statistiken: `{ deleted: { observations: N, outgoing_relations: N, incoming_relations: N } }`

Beispiel:
```json
{ "action": "delete_entity", "entity_id": "ENTITY_ID" }
```

Liefert L√∂schstatistiken, die genau zeigen, was entfernt wurde.

## Technische Highlights

### Duales Zeitstempel-Format (v1.9)

Alle Schreiboperationen (`create_entity`, `add_observation`, `create_relation`) geben Zeitstempel in beiden Formaten zur√ºck:
- `created_at`: Unix-Mikrosekunden (natives CozoDB-Format, pr√§zise f√ºr Berechnungen)
- `created_at_iso`: ISO 8601 String (menschenlesbar, z.B. `"2026-02-28T17:21:19.343Z"`)

Dieses duale Format bietet maximale Flexibilit√§t - verwende Unix-Zeitstempel f√ºr Zeitberechnungen und Vergleiche, oder ISO-Strings f√ºr Anzeige und Logging.

Beispiel-Antwort:
```json
{
  "id": "...",
  "created_at": 1772299279343000,
  "created_at_iso": "2026-02-28T17:21:19.343Z",
  "status": "Entity created"
}
```

### Local ONNX Embeddings (Transformers)

Default-Modell: `Xenova/bge-m3` (1024 Dimensionen).

Die Embeddings werden auf der CPU verarbeitet, um maximale Kompatibilit√§t zu gew√§hrleisten. Sie werden in einem LRU-Cache gehalten (1000 Eintr√§ge, 1h TTL). Bei Embedding-Fehlern wird ein Nullvektor zur√ºckgegeben, damit Tool-Aufrufe stabil bleiben.

### Tiny Learned Reranker (Cross-Encoder)

F√ºr maximale Pr√§zision integriert CozoDB Memory einen spezialisierten **Cross-Encoder Reranker** (Phase 2 RAG).

- **Modell**: `Xenova/ms-marco-MiniLM-L-6-v2` (Lokal via ONNX)
- **Mechanismus**: Nach dem initialen Hybrid-Retrieval werden die Top-Kandidaten (bis zu 30) durch den Cross-Encoder neu bewertet. Im Gegensatz zu Bi-Encodern (Vektoren) verarbeiten Cross-Encoder Query und Dokument simultan und erfassen so tiefere semantische Nuancen.
- **Latenz**: Minimaler Overhead (~4-6ms f√ºr Top 10 Kandidaten).
- **Unterst√ºtzte Tools**: Verf√ºgbar als `rerank: true` Parameter in `search`, `advancedSearch`, `graph_rag` und `agentic_search`.

### Hybrid Search (Vector + Keyword + Graph + Inference) + RRF

Die Suche kombiniert:
- Vektor-√Ñhnlichkeit √ºber HNSW Indizes (`~entity:semantic`, `~observation:semantic`)
- Keyword Matching via Regex (`regex_matches(...)`)
- Graph-Signal √ºber PageRank (f√ºr zentrale Entities)
- Community Expansion: Entities aus der Community der Top Seeds werden mit einem Boost eingebracht
- Inference-Signal: probabilistische Kandidaten (z. B. `expert_in`) mit `confidence` als Score

Fusion: Reciprocal Rank Fusion (RRF) √ºber Quellen `vector`, `keyword`, `graph`, `community`, `inference`.

Zeitlicher Decay (standardm√§√üig aktiv):
- Vor der RRF-Fusion werden Scores zeitbasiert ged√§mpft, basierend auf `created_at` (Validity).
- Halbwertszeit: 90 Tage (exponentieller Decay), mit Source-spezifischen Floors:
  - `keyword`: kein Decay (entspricht ‚Äûexplizit gesucht‚Äú)
  - `graph`/`community`: mindestens 0.6
  - `vector`: mindestens 0.2

Uncertainty/Transparenz:
- Inferenz-Kandidaten werden als `source: "inference"` markiert und liefern eine kurze Begr√ºndung (Unsch√§rfe-Hinweis) im Ergebnis.
- Im `context`-Output wird f√ºr inferierte Entities zus√§tzlich ein `uncertainty_hint` mitgeliefert, damit ein LLM ‚Äûharter Fakt‚Äú vs. ‚ÄûVermutung‚Äú unterscheiden kann.

### Inference Engine

Inference nutzt mehrere Strategien (nicht persistierend):
- **Co-occurrence**: Entit√§tsnamen in Observation-Texten (`related_to`, confidence 0.7)
- **Semantische N√§he**: √§hnliche Entities via HNSW (`similar_to`, bis max. 0.9)
- **Transitivit√§t**: A‚ÜíB und B‚ÜíC (`potentially_related`, confidence 0.5)
- **Expertise-Regel**: `Person` + `works_on` + `uses_tech` ‚áí `expert_in` (confidence 0.7)
- **Query-Triggered Expertise**: Bei Suchanfragen mit Keywords wie `expert`, `skill`, `knowledge`, `competence` etc. wird automatisch eine dedizierte Expertensuche √ºber das Graph-Netzwerk gestartet.

## Optional: HTTP API Bridge

### API Bridge

F√ºr UI/Tools gibt es einen Express-Server, der den `MemoryServer` direkt einbettet.

Start:

```bash
npm run bridge
```

Konfiguration:
- Port √ºber `PORT` (Default: `3001`)

Ausgew√§hlte Endpoints (Prefix `/api`):
- `GET /entities`, `POST /entities`, `GET /entities/:id`, `DELETE /entities/:id`
- `POST /observations`
- `GET /search`, `GET /context`
- `GET /health`
- `GET /snapshots`, `POST /snapshots`

## Entwicklung

### Struktur
- `src/index.ts`: MCP-Server + Tool-Registrierung + Schema Setup
- `src/embedding-service.ts`: Embedding Pipeline + LRU Cache
- `src/hybrid-search.ts`: Suchstrategien + RRF + Community Expansion
- `src/inference-engine.ts`: Inference Strategien
- `src/api_bridge.ts`: Express API Bridge (f√ºr UI)

### Scripts (Root)
- `npm run build`: TypeScript Build
- `npm run dev`: ts-node Start des MCP Servers
- `npm run start`: Startet `dist/index.js` (stdio)
- `npm run bridge`: Build + Start der API Bridge (`dist/api_bridge.js`)
- `npm run benchmark`: F√ºhrt Performance-Tests durch

## User Preference Profiling (Mem0-Style)

Das System pflegt ein persistentes Profil √ºber den Benutzer (Vorlieben, Abneigungen, Arbeitsstil) √ºber die spezialisierte Entit√§t `global_user_profile`.

- **Vorteil**: Personalisierung ohne manuelle Suche ("Ich wei√ü, dass du TypeScript bevorzugst").
- **Funktionsweise**: Alle Beobachtungen, die dieser Entit√§t zugeordnet werden, erhalten bei Such- und Kontext-Anfragen einen signifikanten Boost.
- **Initialisierung**: Das Profil wird beim ersten Start automatisch angelegt.

### Manuelle Profil-Bearbeitung

Du kannst das User-Profil jetzt direkt mit dem `edit_user_profile` MCP-Tool bearbeiten:

```typescript
// Aktuelles Profil anzeigen
{ }

// Metadata aktualisieren
{ 
  metadata: { timezone: "Europe/Berlin", language: "de" } 
}

// Pr√§ferenzen hinzuf√ºgen
{ 
  observations: [
    { text: "Bevorzugt TypeScript gegen√ºber JavaScript" },
    { text: "Mag pr√§gnante Dokumentation" }
  ]
}

// Pr√§ferenzen zur√ºcksetzen
{ 
  clear_observations: true,
  observations: [{ text: "Neue Pr√§ferenz" }]
}

// Name und Typ aktualisieren
{ 
  name: "Entwickler-Profil", 
  type: "UserProfile" 
}
```

**Hinweis**: Du kannst weiterhin die implizite Methode √ºber `mutate_memory` mit `action='add_observation'` und `entity_id='global_user_profile'` verwenden.

### Manuelle Tests

Es gibt verschiedene Test-Skripte f√ºr unterschiedliche Features:

```bash
# Testet Edge-Cases und Basis-Operationen
npx ts-node src/test-edge-cases.ts

# Testet Hybrid-Suche und Kontext-Retrieval
npx ts-node src/test-context.ts

# Testet die Memory-Reflexion (ben√∂tigt Ollama)
npx ts-node test-reflection.ts

# Testet das User Preference Profiling und den Search-Boost
npx ts-node test-user-pref.ts

# Testet die manuelle User-Profil-Bearbeitung
npx ts-node src/test-user-profile.ts
```

## Troubleshooting

### H√§ufige Probleme

**Erster Start dauert lange**
- Der Embedding-Model-Download dauert beim ersten Start 30-90 Sekunden (Transformers l√§dt ~500MB Artefakte)
- Dies ist normal und passiert nur einmal
- Nachfolgende Starts sind schnell (< 2 Sekunden)

**Cleanup/Reflect ben√∂tigt Ollama**
- Bei Verwendung von `cleanup` oder `reflect` Aktionen muss ein Ollama-Dienst lokal laufen
- Ollama installieren von https://ollama.ai
- Gew√ºnschtes Modell pullen: `ollama pull demyagent-4b-i1:Q6_K` (oder bevorzugtes Modell)

**Windows-Spezifisch**
- Embeddings werden auf der CPU verarbeitet f√ºr maximale Kompatibilit√§t
- RocksDB-Backend ben√∂tigt Visual C++ Redistributable bei Verwendung dieser Option

**Performance-Probleme**
- Erste Query nach Neustart ist langsamer (kalter Cache)
- `health` Aktion nutzen um Cache-Hit-Raten zu pr√ºfen
- RocksDB-Backend erw√§gen f√ºr Datasets > 100k Entities

## Lizenz

Dieses Projekt ist unter der Apache-2.0 Lizenz lizenziert. Siehe die [LICENSE](LICENSE) Datei f√ºr Details.

## Haftungsausschluss

Single-User, Local-First: Dieses Projekt wurde entwickelt, um auf einem einzelnen Benutzer und einer lokalen Installation zu funktionieren.